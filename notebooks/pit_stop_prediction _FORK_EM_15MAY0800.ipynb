{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333f740",
   "metadata": {},
   "source": [
    "# **F1 Pit Stop Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387603e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, regexp_replace, col, when, to_timestamp, lead, avg, stddev, lag, max, sum, first, last, split, coalesce, lit\n",
    "from pyspark.sql.types import IntegerType, BooleanType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import optuna\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"scripts\")))\n",
    "from constants import LAPS, TELEMETRY\n",
    "# from preprocessing import add_pit_stop_label, engineer_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ad450",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db9391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lap Data Aggregation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"24g\") \\\n",
    "    .config(\"spark.executor.memory\", \"24g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819e3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_telemetry_files = glob.glob(os.path.join(TELEMETRY, \"*.csv\"))\n",
    "all_laps_files = glob.glob(os.path.join(LAPS, \"*.csv\"))\n",
    "\n",
    "telemetry_data = spark.read.option(\"header\", True).csv(all_telemetry_files)\n",
    "lap_data = spark.read.option(\"header\", True).csv(all_laps_files)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdb59c70",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Load all CSVs into a Spark DataFrame\n",
    "lap_data = spark.read.option(\"header\", True).csv(os.path.join(LAPS, \"*.csv\"))\n",
    "telemetry_data = spark.read.option(\"header\", True).csv(os.path.join(TELEMETRY, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946ae3e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8851a65",
   "metadata": {},
   "source": [
    "### Lap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb59a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the file name from the file path\n",
    "file_name_col = input_file_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e9cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the event name and session from the file name\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"Year\", regexp_extract(file_name_col, r\"/(\\d{4})_[^/]+_[QR]\\.csv$\", 1))\n",
    "    .withColumn(\"EventName\", regexp_replace(regexp_extract(file_name_col, r\"/\\d{4}_(.+)_[QR]\\.csv$\", 1), \"_\", \" \"))\n",
    "    .withColumn(\"Session\", regexp_extract(file_name_col, r\"/\\d{4}_[^/]+_([QR])\\.csv$\", 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f827b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql view\n",
    "lap_data.createOrReplaceTempView(\"laps\")\n",
    "\n",
    "# Filter for only Race sessions\n",
    "lap_data = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM laps\n",
    "    WHERE Session = 'R'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7abdd",
   "metadata": {},
   "source": [
    "**Fixing Datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8454b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check datatypes\n",
    "# lap_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7142a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datatypes\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"LapSessionTime\", regexp_replace(col(\"Time\"), r\"^0 days \", \"\"))\n",
    "    .withColumn(\"DriverNumber\", col(\"DriverNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"LapTime\", split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapNumber\", col(\"LapNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"Stint\", col(\"Stint\").cast(IntegerType()))\n",
    "    .withColumn(\"PitOutTime\", split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"PitInTime\", split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector1Time\", split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector2Time\", split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector3Time\", split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector1SessionTime\", split(regexp_replace(col(\"Sector1SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector1SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector1SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector2SessionTime\", split(regexp_replace(col(\"Sector2SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector2SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector2SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector3SessionTime\", split(regexp_replace(col(\"Sector3SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector3SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector3SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"SpeedI1\", col(\"SpeedI1\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedI2\", col(\"SpeedI2\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedFL\", col(\"SpeedFL\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedST\", col(\"SpeedST\").cast(IntegerType()))\n",
    "    .withColumn(\"IsPersonalBest\", col(\"IsPersonalBest\").cast(BooleanType()))\n",
    "    .withColumn(\"TyreLife\", col(\"TyreLife\").cast(IntegerType()))\n",
    "    .withColumn(\"FreshTyre\", col(\"FreshTyre\").cast(BooleanType()))\n",
    "    .withColumn(\"LapStartTime\", split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapStartDate\", to_timestamp(\"LapStartDate\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    .withColumn(\"TrackStatus\", col(\"TrackStatus\").cast(IntegerType()))\n",
    "    .withColumn(\"Position\", col(\"Position\").cast(IntegerType()))\n",
    "    .withColumn(\"Deleted\", col(\"Deleted\").cast(BooleanType()))\n",
    "    .withColumn(\"FastF1Generated\", col(\"FastF1Generated\").cast(BooleanType()))\n",
    "    .withColumn(\"IsAccurate\", col(\"IsAccurate\").cast(BooleanType()))\n",
    "    .withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
    "    .withColumn(\"LapSessionTime\", split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    ")\n",
    "\n",
    "lap_data = lap_data.drop(col(\"Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1fac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the result\n",
    "# lap_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ace846",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf73b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define windows\n",
    "start_position_window = Window.partitionBy(\"Year\", \"EventName\", \"Driver\")\n",
    "lap_order_window = start_position_window.orderBy(\"LapNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0524bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"rolling_avg_laptime\", avg(\"LapTime\").over(lap_order_window.rowsBetween(Window.unboundedPreceding, 0)))\n",
    "    .withColumn(\"pit_in_lap\", when(col(\"PitInTime\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"pit_exit_lap\", when(col(\"PitOutTime\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\n",
    "        \"last_pit_lap\",\n",
    "        coalesce(\n",
    "            max(\"pit_exit_lap\").over(lap_order_window.rowsBetween(Window.unboundedPreceding, 0)),\n",
    "            lit(0)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"laps_since_last_pit\", col(\"LapNumber\") - col(\"last_pit_lap\"))\n",
    "    .withColumn(\n",
    "        \"prev_compound\", \n",
    "        when(\n",
    "            col(\"LapNumber\") == 1, col(\"Compound\")\n",
    "        ).otherwise(\n",
    "            lag(\"Compound\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pit_stop_duration\",\n",
    "        when(\n",
    "            col(\"PitOutTime\").isNull(),\n",
    "            lit(0)\n",
    "        ).otherwise(\n",
    "            col(\"PitOutTime\") - lag(\"PitInTime\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"max_pit_stop_duration\", max(\"pit_stop_duration\").over(lap_order_window))\n",
    "    .withColumn(\"start_position\", first(when(col(\"LapNumber\") == 1, col(\"Position\")), ignorenulls=True).over(start_position_window))\n",
    "    .withColumn(\"position_change_since_race_start\", col(\"start_position\") - col(\"Position\"))\n",
    "    .withColumn(\n",
    "        \"fastest_sector\", when(\n",
    "            (col(\"Sector1Time\") <= col(\"Sector2Time\")) & (col(\"Sector1Time\") <= col(\"Sector3Time\")), 1\n",
    "        ).when(\n",
    "            (col(\"Sector2Time\") <= col(\"Sector1Time\")) & (col(\"Sector2Time\") <= col(\"Sector3Time\")), 2\n",
    "        ).otherwise(3)\n",
    "    )\n",
    ")\n",
    "\n",
    "lap_data = lap_data.drop(\"Sector1SessionTime\", \"Sector2SessionTime\", \"Sector3SessionTime\", \"DeletedReason\", \"IsAccurate\", \"LapStartDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48d5b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+--------+------------+-----------+--------+-------+---------------+----+--------------------+-------+--------------+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "|Driver|DriverNumber|LapTime|LapNumber|Stint|PitOutTime|PitInTime|Sector1Time|Sector2Time|Sector3Time|SpeedI1|SpeedI2|SpeedFL|SpeedST|IsPersonalBest|Compound|TyreLife|FreshTyre|    Team|LapStartTime|TrackStatus|Position|Deleted|FastF1Generated|Year|           EventName|Session|LapSessionTime|rolling_avg_laptime|pit_in_lap|pit_exit_lap|last_pit_lap|laps_since_last_pit|prev_compound|pit_stop_duration|max_pit_stop_duration|start_position|position_change_since_race_start|fastest_sector|\n",
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+--------+------------+-----------+--------+-------+---------------+----+--------------------+-------+--------------+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "|   ALB|          23|101.738|        1|    1|      NULL|     NULL|       NULL|     40.201|     36.214|    291|    298|    212|    270|         false|  MEDIUM|       1|     true|Williams|    3730.161|          1|      17|  false|          false|2022|Abu Dhabi Grand Prix|      R|      3832.133|            101.738|         0|           0|           0|                  1|       MEDIUM|              0.0|                  0.0|            17|                               0|             3|\n",
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+--------+------------+-----------+--------+-------+---------------+----+--------------------+-------+--------------+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lap_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd1d0c",
   "metadata": {},
   "source": [
    "### Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63af09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the file name from the file path\n",
    "file_name_col = input_file_name()\n",
    "\n",
    "# Extract the event name and session from the file name\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"Year\", regexp_extract(file_name_col, r\"/(\\d{4})_[^/]+_[QR]\\.csv$\", 1))\n",
    "    .withColumn(\"EventName\", regexp_replace(regexp_extract(file_name_col, r\"/\\d{4}_(.+)_[QR]\\.csv$\", 1), \"_\", \" \"))\n",
    "    .withColumn(\"Session\", regexp_extract(file_name_col, r\"/\\d{4}_[^/]+_([QR])\\.csv$\", 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f14c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql view\n",
    "telemetry_data.createOrReplaceTempView(\"telemetry\")\n",
    "\n",
    "# Filter for only Race events\n",
    "telemetry_data = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM telemetry\n",
    "    WHERE Session = 'R'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451a441",
   "metadata": {},
   "source": [
    "**Fixing Datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2936752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check datatypes\n",
    "# telemetry_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0660d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"Date\", to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    .withColumn(\"RPM\", col(\"RPM\").cast(IntegerType()))\n",
    "    .withColumn(\"Speed\", col(\"Speed\").cast(IntegerType()))\n",
    "    .withColumn(\"nGear\", col(\"nGear\").cast(IntegerType()))\n",
    "    .withColumn(\"Throttle\", col(\"Throttle\").cast(IntegerType()))\n",
    "    .withColumn(\"Brake\", col(\"Brake\").cast(BooleanType()).cast(IntegerType()))\n",
    "    .withColumn(\"DRS\", col(\"DRS\").cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"DataCollectionTime\", split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"SessionTime\", split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Distance\", col(\"Distance\").cast(FloatType()))\n",
    "    .withColumn(\"LapNumber\", col(\"LapNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"IsDRSActive\", when(\n",
    "            col(\"DRS\").isin(10, 12, 14), 1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "telemetry_data = telemetry_data.drop(col(\"Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095adaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-----+--------+-----+---+------+-----------+--------+------+---------+----+----------------+-------+------------------+-----------+\n",
      "|                Date|  RPM|Speed|nGear|Throttle|Brake|DRS|Source|SessionTime|Distance|Driver|LapNumber|Year|       EventName|Session|DataCollectionTime|IsDRSActive|\n",
      "+--------------------+-----+-----+-----+--------+-----+---+------+-----------+--------+------+---------+----+----------------+-------+------------------+-----------+\n",
      "|2023-08-27 13:03:...|10093|    0|    1|      15|    0|  1|   car|   3725.042|     0.0|   VER|        1|2023|Dutch Grand Prix|      R|             0.082|          0|\n",
      "+--------------------+-----+-----+-----+--------+-----+---+------+-----------+--------+------+---------+----+----------------+-------+------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Show the result\n",
    "telemetry_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f788ef",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d332796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window\n",
    "window_spec = Window.partitionBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").orderBy(\"SessionTime\")\n",
    "last_50_window = window_spec.rowsBetween(-49, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0f24cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-lap aggregates\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"avg_speed_last_lap\", avg(\"Speed\").over(window_spec))\n",
    "    .withColumn(\"max_speed_last_lap\", max(\"Speed\").over(window_spec))\n",
    "    .withColumn(\"avg_throttle_last_lap\", avg(\"Throttle\").over(window_spec))\n",
    "    .withColumn(\"avg_brake_last_lap\", avg(\"Brake\").over(window_spec))\n",
    "    .withColumn(\"avg_rpm\", avg(\"RPM\").over(window_spec))\n",
    "    .withColumn(\"gear_change\", when(col(\"nGear\") != lag(\"nGear\").over(window_spec), 1).otherwise(0))\n",
    "    .withColumn(\"gear_change_count\", sum(\"gear_change\").over(window_spec))\n",
    "    .withColumn(\n",
    "        \"DRS_activation_count\",\n",
    "        sum(\n",
    "            when(\n",
    "                (~lag(\"DRS\").over(window_spec).isin(10, 12, 14)) & (col(\"DRS\").isin(10, 12, 14)),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2fb68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling features over last 50 telemetry rows\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"rolling_throttle_mean\", avg(\"Throttle\").over(last_50_window))\n",
    "    .withColumn(\"rolling_brake_intensity\", avg(\"Brake\").over(last_50_window))\n",
    "    .withColumn(\"rolling_gear_change\", when(col(\"nGear\") != lag(\"nGear\").over(window_spec), 1).otherwise(0))\n",
    "    .withColumn(\"rolling_gear_change_rate\", avg(\"rolling_gear_change\").over(last_50_window))\n",
    "    .withColumn(\"rolling_speed_mean\", avg(\"Speed\").over(last_50_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9288123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sector features (define final 5% of distance per lap)\n",
    "max_distance = telemetry_data.groupBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").agg(max(\"Distance\").alias(\"max_dist\"))\n",
    "telemetry_data = telemetry_data.join(max_distance, on=[\"Year\", \"EventName\", \"Driver\", \"LapNumber\"])\n",
    "telemetry_data = telemetry_data.withColumn(\"in_final_sector\", col(\"Distance\") >= col(\"max_dist\") * 0.95)\n",
    "\n",
    "# Define new window\n",
    "final_sector_window = Window.partitionBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"final_sector_avg_speed\", avg(when(col(\"in_final_sector\"), col(\"Speed\"))).over(final_sector_window))\n",
    "    .withColumn(\"final_sector_throttle\", avg(when(col(\"in_final_sector\"), col(\"Throttle\"))).over(final_sector_window))\n",
    "    .withColumn(\"final_sector_brake\", avg(when(col(\"in_final_sector\"), col(\"Brake\"))).over(final_sector_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "094b2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telemetry_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d5ac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final per-lap features\n",
    "lap_feature_cols = [\n",
    "    \"EventName\", \"Driver\", \"LapNumber\", \"Year\", \"Session\",\n",
    "    \"avg_speed_last_lap\", \"max_speed_last_lap\",\n",
    "    \"avg_throttle_last_lap\", \"avg_brake_last_lap\",\n",
    "    \"gear_change_count\", \"avg_rpm\",\n",
    "    \"rolling_throttle_mean\", \"rolling_brake_intensity\",\n",
    "    \"rolling_gear_change_rate\", \"rolling_speed_mean\",\n",
    "    \"final_sector_avg_speed\", \"final_sector_throttle\", \n",
    "    \"final_sector_brake\"\n",
    "]\n",
    "\n",
    "# For all columns, take the FIRST value per (Driver, LapNumber)\n",
    "# Because window functions already populated each row with the same value within each lap\n",
    "aggregated_laps = (\n",
    "    telemetry_data\n",
    "    .select(*lap_feature_cols)\n",
    "    .groupBy(\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\")\n",
    "    .agg(*[\n",
    "        first(col_name).alias(col_name) \n",
    "        if col_name != \"DRS_activation_count\" \n",
    "        else last(col_name).alias(col_name) \n",
    "        for col_name in lap_feature_cols \n",
    "        if col_name not in (\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4892b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_laps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c6a39",
   "metadata": {},
   "source": [
    "### Joining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join lap_data and telemetry_data\n",
    "data = (\n",
    "    lap_data\n",
    "    .alias('lap')\n",
    "    .join(\n",
    "        aggregated_laps.alias('telemetry')\n",
    "        ,on=[\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\"]\n",
    "        ,how=\"outer\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec477f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"WillPitNextLap\", when(\n",
    "            lead(\"PitInTime\", 1).over(Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\").orderBy(\"LapNumber\")).isNotNull(), 1\n",
    "        )\n",
    "    .otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# data = data.drop(\"PitInTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65edc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- EventName: string (nullable = true)\n",
      " |-- Session: string (nullable = true)\n",
      " |-- Driver: string (nullable = true)\n",
      " |-- LapNumber: integer (nullable = true)\n",
      " |-- DriverNumber: integer (nullable = true)\n",
      " |-- LapTime: double (nullable = true)\n",
      " |-- Stint: integer (nullable = true)\n",
      " |-- PitOutTime: double (nullable = true)\n",
      " |-- PitInTime: double (nullable = true)\n",
      " |-- Sector1Time: double (nullable = true)\n",
      " |-- Sector2Time: double (nullable = true)\n",
      " |-- Sector3Time: double (nullable = true)\n",
      " |-- SpeedI1: integer (nullable = true)\n",
      " |-- SpeedI2: integer (nullable = true)\n",
      " |-- SpeedFL: integer (nullable = true)\n",
      " |-- SpeedST: integer (nullable = true)\n",
      " |-- IsPersonalBest: boolean (nullable = true)\n",
      " |-- Compound: string (nullable = true)\n",
      " |-- TyreLife: integer (nullable = true)\n",
      " |-- FreshTyre: boolean (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- LapStartTime: double (nullable = true)\n",
      " |-- TrackStatus: integer (nullable = true)\n",
      " |-- Position: integer (nullable = true)\n",
      " |-- Deleted: boolean (nullable = true)\n",
      " |-- FastF1Generated: boolean (nullable = true)\n",
      " |-- LapSessionTime: double (nullable = true)\n",
      " |-- rolling_avg_laptime: double (nullable = true)\n",
      " |-- pit_in_lap: integer (nullable = true)\n",
      " |-- pit_exit_lap: integer (nullable = true)\n",
      " |-- last_pit_lap: integer (nullable = true)\n",
      " |-- laps_since_last_pit: integer (nullable = true)\n",
      " |-- prev_compound: string (nullable = true)\n",
      " |-- pit_stop_duration: double (nullable = true)\n",
      " |-- max_pit_stop_duration: double (nullable = true)\n",
      " |-- start_position: integer (nullable = true)\n",
      " |-- position_change_since_race_start: integer (nullable = true)\n",
      " |-- fastest_sector: integer (nullable = true)\n",
      " |-- avg_speed_last_lap: double (nullable = true)\n",
      " |-- max_speed_last_lap: integer (nullable = true)\n",
      " |-- avg_throttle_last_lap: double (nullable = true)\n",
      " |-- avg_brake_last_lap: double (nullable = true)\n",
      " |-- gear_change_count: long (nullable = true)\n",
      " |-- avg_rpm: double (nullable = true)\n",
      " |-- rolling_throttle_mean: double (nullable = true)\n",
      " |-- rolling_brake_intensity: double (nullable = true)\n",
      " |-- rolling_gear_change_rate: double (nullable = true)\n",
      " |-- rolling_speed_mean: double (nullable = true)\n",
      " |-- final_sector_avg_speed: double (nullable = true)\n",
      " |-- final_sector_throttle: double (nullable = true)\n",
      " |-- final_sector_brake: double (nullable = true)\n",
      " |-- WillPitNextLap: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fde46b",
   "metadata": {},
   "source": [
    "**Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute null counts\n",
    "# null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# # Convert to a Row to filter in Python\n",
    "# null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# # Filter and print only columns with nulls\n",
    "# for col_name, count in null_counts_dict.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6faa6f1",
   "metadata": {},
   "source": [
    "LapTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab15d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check rows with missing values\n",
    "# data.filter(col(\"LapTime\").isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d06d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with NULL LapTime and Sector<1, 2, 3>Time are DNF so we drop these rows\n",
    "data = (\n",
    "    data\n",
    "    .filter(~(\n",
    "        col(\"Sector1Time\").isNull() & \n",
    "        col(\"Sector2Time\").isNull() & \n",
    "        col(\"Sector3Time\").isNull() & \n",
    "        col(\"LapTime\").isNull()\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove SectorTime columns\n",
    "data = data.drop(\"Sector1Time\", \"Sector2Time\", \"Sector3Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067cc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values - compute by subtracting the time at the end and at the start of the lap\n",
    "data = data.withColumn(\"LapTime\", col(\"LapSessionTime\") - col(\"LapStartTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdfc1f",
   "metadata": {},
   "source": [
    "Missing values recheck, since DNF rows were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute null counts\n",
    "# null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# # Convert to a Row to filter in Python\n",
    "# null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# # Filter and print only columns with nulls\n",
    "# for col_name, count in null_counts_dict.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836904c",
   "metadata": {},
   "source": [
    "SpeedI1, SpeedI2, SpeedFL, SpeedST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66719c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - speed rolling average\n",
    "driver_lap_window = Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\").orderBy(\"LapNumber\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# List of columns to process\n",
    "speed_cols = [\"SpeedI1\", \"SpeedI2\", \"SpeedFL\", \"SpeedST\"]\n",
    "\n",
    "# Fill missing values\n",
    "for col_name in speed_cols:\n",
    "    cumulative_avg = avg(col(col_name)).over(driver_lap_window)\n",
    "    data = (\n",
    "        data\n",
    "        .withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull(), cumulative_avg).otherwise(col(col_name))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - teammate's speed in same lap\n",
    "\n",
    "# Self-join on teammate info\n",
    "teammate_join = data.alias(\"self\").join(\n",
    "    data.alias(\"tm\"),\n",
    "    on=[\n",
    "        col(\"self.Year\") == col(\"tm.Year\"),\n",
    "        col(\"self.EventName\") == col(\"tm.EventName\"),\n",
    "        col(\"self.Session\") == col(\"tm.Session\"),\n",
    "        col(\"self.Team\") == col(\"tm.Team\"),\n",
    "        col(\"self.LapNumber\") == col(\"tm.LapNumber\"),\n",
    "        col(\"self.Driver\") != col(\"tm.Driver\")\n",
    "    ],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Replace missing values from teammate values\n",
    "updated_cols = [\n",
    "    coalesce(col(f\"self.{col_name}\"), col(f\"tm.{col_name}\")).alias(col_name)\n",
    "    if col_name in speed_cols else col(f\"self.{col_name}\")\n",
    "    for col_name in data.columns\n",
    "]\n",
    "\n",
    "# Assign back to `data` (replacing the original one)\n",
    "data = teammate_join.select(*updated_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - finish line speed with longest straight speed\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"SpeedFL\",\n",
    "        when(col(\"SpeedFL\").isNull(), col(\"SpeedST\")).otherwise(col(\"SpeedFL\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0246f",
   "metadata": {},
   "source": [
    "pit_stop_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1093e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"pit_stop_duration\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7abdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"pit_stop_duration\").isNull()\n",
    "# ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db32852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values seem to be mistakes, so we set PitOutTime to NULL and recompute pit_stop_duration and max_pit_stop_duration\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"PitOutTime\",\n",
    "        when(col(\"pit_stop_duration\").isNull(), None).otherwise(col(\"PitOutTime\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pit_stop_duration\",\n",
    "        when(\n",
    "            col(\"PitOutTime\").isNull(),\n",
    "            lit(0)\n",
    "        ).otherwise(\n",
    "            col(\"PitOutTime\") - lag(\"PitInTime\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"max_pit_stop_duration\", max(\"pit_stop_duration\").over(lap_order_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"pit_stop_duration\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43db11",
   "metadata": {},
   "source": [
    "max_pit_stop_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca96008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"max_pit_stop_duration\").isNull()\n",
    "# ).count()\n",
    "\n",
    "# # No more :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute null counts\n",
    "# null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# # Convert to a Row to filter in Python\n",
    "# null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# # Filter and print only columns with nulls\n",
    "# for col_name, count in null_counts_dict.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef11b74",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42778efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_data = data.filter(~((col(\"EventName\") == \"Abu Dhabi Grand Prix\") & (col(\"Year\") == 2023)) & ~((col(\"EventName\") == \"Las Vegas Grand Prix\") & (col(\"Year\") == 2023)))\n",
    "val_data = data.filter((col(\"EventName\") == \"Las Vegas Grand Prix\") & (col(\"Year\") == 2023))\n",
    "test_data = data.filter((col(\"EventName\") == \"Abu Dhabi Grand Prix\") & (col(\"Year\") == 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, ChiSqSelector\n",
    "from pyspark.sql.functions import col\n",
    "import uuid\n",
    "\n",
    "def train_model(target, train_data, val_data, test_data, model_type, optimize, num_features=20, n_trials=2):\n",
    "    \"\"\"\n",
    "    Train a model with optional Optuna hyperparameter optimization, feature selection, and scaling for LR/MLP.\n",
    "    \n",
    "    Args:\n",
    "        target (str): Target column name.\n",
    "        train_data: PySpark DataFrame for training.\n",
    "        val_data: PySpark DataFrame for validation during optimization.\n",
    "        test_data: PySpark DataFrame for final evaluation.\n",
    "        model_type (str): Model type ('xgb', 'rf', 'lr', 'mlp').\n",
    "        optimize (bool): If True, optimize hyperparameters with Optuna; if False, use defaults.\n",
    "        num_features (int): Number of features to select using Chi-Square Selector.\n",
    "        n_trials (int): Number of Optuna trials if optimize=True.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained model, areaUnderPR score on test data)\n",
    "    \"\"\"\n",
    "    # Validate model_type\n",
    "    valid_models = ['xgb', 'rf', 'lr', 'mlp']\n",
    "    if model_type not in valid_models:\n",
    "        raise ValueError(f\"model_type must be one of {valid_models}\")\n",
    "\n",
    "    # Define indexers for categorical features\n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=\"Team\", outputCol=\"TeamIndex\"),\n",
    "        StringIndexer(inputCol=\"Compound\", outputCol=\"CompoundIndex\"),\n",
    "        StringIndexer(inputCol=\"Driver\", outputCol=\"DriverIndex\"),\n",
    "        StringIndexer(inputCol=\"EventName\", outputCol=\"EventNameIndex\")\n",
    "    ]\n",
    "\n",
    "    # Cache data\n",
    "    train_data.cache()\n",
    "    val_data.cache()\n",
    "    test_data.cache()\n",
    "\n",
    "    # Apply indexers to all datasets\n",
    "    indexer_pipeline = Pipeline(stages=indexers)\n",
    "    train_data = indexer_pipeline.fit(train_data).transform(train_data)\n",
    "    val_data = indexer_pipeline.fit(val_data).transform(val_data)\n",
    "    test_data = indexer_pipeline.fit(test_data).transform(test_data)\n",
    "\n",
    "    # Get all columns for feature selection (exclude target and raw categorical columns)\n",
    "    categorical_cols = [\"Team\", \"Compound\", \"Driver\", \"EventName\"]\n",
    "    feature_cols = [col for col in train_data.columns if col != target and col not in categorical_cols]\n",
    "    \n",
    "    # Temporary assembler for feature selection\n",
    "    temp_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    temp_pipeline = Pipeline(stages=[temp_assembler])\n",
    "    train_data = temp_pipeline.fit(train_data).transform(train_data)\n",
    "\n",
    "    # Apply Chi-Square Selector\n",
    "    selector = ChiSqSelector(\n",
    "        numTopFeatures=num_features,\n",
    "        featuresCol=\"features\",\n",
    "        outputCol=\"selected_features\",\n",
    "        labelCol=target\n",
    "    )\n",
    "    selector_model = selector.fit(train_data)\n",
    "    selected_indices = selector_model.selectedFeatures\n",
    "    selected_feature_names = [feature_cols[i] for i in selected_indices]\n",
    "    print(f\"Selected {len(selected_feature_names)} features: {selected_feature_names}\")\n",
    "\n",
    "    # Create new assembler with selected features\n",
    "    assembler = VectorAssembler(inputCols=selected_feature_names, outputCol=\"features\")\n",
    "    feature_pipeline = Pipeline(stages=[assembler])\n",
    "\n",
    "    # Update datasets with selected features\n",
    "    train_data = feature_pipeline.fit(train_data).transform(train_data)\n",
    "    val_data = feature_pipeline.fit(val_data).transform(val_data)\n",
    "    test_data = feature_pipeline.fit(test_data).transform(test_data)\n",
    "\n",
    "    def get_classifier(trial=None):\n",
    "        \"\"\"Define classifier based on model_type and optional trial parameters.\"\"\"\n",
    "        if model_type == 'rf':\n",
    "            if optimize and trial:\n",
    "                num_trees = trial.suggest_int(\"numTrees\", 10, 100)\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 5, 30)\n",
    "                min_instances_per_node = trial.suggest_int(\"minInstancesPerNode\", 1, 10)\n",
    "                subsampling_rate = trial.suggest_float(\"subsamplingRate\", 0.5, 1.0)\n",
    "                max_bins = trial.suggest_int(\"maxBins\", 10, 50)\n",
    "                return RandomForestClassifier(labelCol=target, featuresCol=\"features\",\n",
    "                                            numTrees=num_trees, maxDepth=max_depth,\n",
    "                                            minInstancesPerNode=min_instances_per_node,\n",
    "                                            subsamplingRate=subsampling_rate, maxBins=max_bins)\n",
    "            return RandomForestClassifier(labelCol=target, featuresCol=\"features\")\n",
    "        \n",
    "        elif model_type == 'xgb':\n",
    "            if optimize and trial:\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 3, 10)\n",
    "                num_round = trial.suggest_int(\"num_round\", 10, 100)\n",
    "                eta = trial.suggest_float(\"eta\", 0.01, 0.3)\n",
    "                subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "                colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "                return SparkXGBClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"features\",\n",
    "                    max_depth=max_depth,\n",
    "                    num_round=num_round,\n",
    "                    eta=eta,\n",
    "                    subsample=subsample,\n",
    "                    colsample_bytree=colsample_bytree\n",
    "                )\n",
    "            return SparkXGBClassifier(labelCol=target, featuresCol=\"features\")\n",
    "        \n",
    "        elif model_type == 'lr':\n",
    "            if optimize and trial:\n",
    "                reg_param = trial.suggest_float(\"regParam\", 1e-5, 0.5, log=True)\n",
    "                elastic_net = trial.suggest_float(\"elasticNetParam\", 0.0, 1.0)\n",
    "                tol = trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 300)\n",
    "                fit_intercept = trial.suggest_categorical(\"fitIntercept\", [True, False])\n",
    "                return LogisticRegression(labelCol=target, featuresCol=\"scaled_features\",\n",
    "                                        regParam=reg_param, elasticNetParam=elastic_net,\n",
    "                                        tol=tol, maxIter=max_iter, fitIntercept=fit_intercept)\n",
    "            return LogisticRegression(labelCol=target, featuresCol=\"scaled_features\")\n",
    "        \n",
    "        elif model_type == 'mlp':\n",
    "            layers = [train_data.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"], 64, 32, 2]  \n",
    "            if optimize and trial:\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 200)\n",
    "                block_size = trial.suggest_int(\"blockSize\", 32, 128)\n",
    "                step_size = trial.suggest_float(\"stepSize\", 0.001, 0.1, log=True)\n",
    "                return MultilayerPerceptronClassifier(labelCol=target, featuresCol=\"scaled_features\",\n",
    "                                                    layers=layers, maxIter=max_iter, blockSize=block_size,\n",
    "                                                    stepSize=step_size)\n",
    "            return MultilayerPerceptronClassifier(labelCol=target, featuresCol=\"scaled_features\",\n",
    "                                                layers=layers)\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Objective function for Optuna optimization using validation data.\"\"\"\n",
    "        classifier = get_classifier(trial)\n",
    "        # Add scaler for lr/mlp\n",
    "        stages = [assembler]\n",
    "        if model_type in ['lr', 'mlp']:\n",
    "            scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "            stages.append(scaler)\n",
    "        stages.append(classifier)\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        try:\n",
    "            model = pipeline.fit(train_data)\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "            auc = evaluator.evaluate(model.transform(val_data))\n",
    "            return auc\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Train model\n",
    "    if optimize:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\", \n",
    "            study_name=f\"{model_type}_optimization_{uuid.uuid4()}\"\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
    "        print(f\"Best areaUnderPR: {study.best_value}\")\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        classifier = get_classifier(study.best_trial)\n",
    "    else:\n",
    "        classifier = get_classifier()\n",
    "\n",
    "    # Build pipeline\n",
    "    stages = [assembler]\n",
    "    if model_type in ['lr', 'mlp']:\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        stages.append(scaler)\n",
    "    stages.append(classifier)\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    # Fit and evaluate on test data\n",
    "    model = pipeline.fit(train_data)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "    auc = evaluator.evaluate(model.transform(test_data))\n",
    "    \n",
    "    # Unpersist data\n",
    "    train_data.unpersist()\n",
    "    val_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "    \n",
    "    return model, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee549e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, auc \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m      2\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWillPitNextLap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[0;32m      4\u001b[0m     val_data\u001b[38;5;241m=\u001b[39mval_data,\n\u001b[0;32m      5\u001b[0m     test_data\u001b[38;5;241m=\u001b[39mtest_data,\n\u001b[0;32m      6\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      9\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel AUC on test data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 47\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(target, train_data, val_data, test_data, model_type, optimize, num_features, n_trials)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Apply indexers to all datasets\u001b[39;00m\n\u001b[0;32m     46\u001b[0m indexer_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39mindexers)\n\u001b[1;32m---> 47\u001b[0m train_data \u001b[38;5;241m=\u001b[39m indexer_pipeline\u001b[38;5;241m.\u001b[39mfit(train_data)\u001b[38;5;241m.\u001b[39mtransform(train_data)\n\u001b[0;32m     48\u001b[0m val_data \u001b[38;5;241m=\u001b[39m indexer_pipeline\u001b[38;5;241m.\u001b[39mfit(val_data)\u001b[38;5;241m.\u001b[39mtransform(val_data)\n\u001b[0;32m     49\u001b[0m test_data \u001b[38;5;241m=\u001b[39m indexer_pipeline\u001b[38;5;241m.\u001b[39mfit(test_data)\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\envs\\ML\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, auc = train_model(\n",
    "    target=\"WillPitNextLap\",\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    model_type=\"xgb\",\n",
    "    optimize=True,\n",
    "    num_features=30,\n",
    "    n_trials=20\n",
    ")\n",
    "print(f\"Model AUC on test data: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
