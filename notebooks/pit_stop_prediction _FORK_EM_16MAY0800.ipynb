{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333f740",
   "metadata": {},
   "source": [
    "# **F1 Pit Stop Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387603e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, regexp_replace, col, when, to_timestamp, lead, avg, stddev, lag, max, sum, first, last, split, coalesce, lit\n",
    "from pyspark.sql.types import IntegerType, BooleanType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import optuna\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"scripts\")))\n",
    "from constants import LAPS, TELEMETRY\n",
    "# from preprocessing import add_pit_stop_label, engineer_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ad450",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db9391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lap Data Aggregation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"24g\") \\\n",
    "    .config(\"spark.executor.memory\", \"24g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819e3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_telemetry_files = glob.glob(os.path.join(TELEMETRY, \"*.csv\"))\n",
    "all_laps_files = glob.glob(os.path.join(LAPS, \"*.csv\"))\n",
    "\n",
    "telemetry_data = spark.read.option(\"header\", True).csv(all_telemetry_files)\n",
    "lap_data = spark.read.option(\"header\", True).csv(all_laps_files)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdb59c70",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Load all CSVs into a Spark DataFrame\n",
    "lap_data = spark.read.option(\"header\", True).csv(os.path.join(LAPS, \"*.csv\"))\n",
    "telemetry_data = spark.read.option(\"header\", True).csv(os.path.join(TELEMETRY, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946ae3e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8851a65",
   "metadata": {},
   "source": [
    "### Lap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb59a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the file name from the file path\n",
    "file_name_col = input_file_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e9cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the event name and session from the file name\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"Year\", regexp_extract(file_name_col, r\"/(\\d{4})_[^/]+_[QR]\\.csv$\", 1))\n",
    "    .withColumn(\"EventName\", regexp_replace(regexp_extract(file_name_col, r\"/\\d{4}_(.+)_[QR]\\.csv$\", 1), \"_\", \" \"))\n",
    "    .withColumn(\"Session\", regexp_extract(file_name_col, r\"/\\d{4}_[^/]+_([QR])\\.csv$\", 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f827b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql view\n",
    "lap_data.createOrReplaceTempView(\"laps\")\n",
    "\n",
    "# Filter for only Race sessions\n",
    "lap_data = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM laps\n",
    "    WHERE Session = 'R'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7abdd",
   "metadata": {},
   "source": [
    "**Fixing Datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8454b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check datatypes\n",
    "# lap_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7142a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datatypes\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"LapSessionTime\", regexp_replace(col(\"Time\"), r\"^0 days \", \"\"))\n",
    "    .withColumn(\"DriverNumber\", col(\"DriverNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"LapTime\", split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapNumber\", col(\"LapNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"Stint\", col(\"Stint\").cast(IntegerType()))\n",
    "    .withColumn(\"PitOutTime\", split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"PitInTime\", split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector1Time\", split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector2Time\", split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector3Time\", split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector1SessionTime\", split(regexp_replace(col(\"Sector1SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector1SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector1SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector2SessionTime\", split(regexp_replace(col(\"Sector2SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector2SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector2SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector3SessionTime\", split(regexp_replace(col(\"Sector3SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector3SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector3SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"SpeedI1\", col(\"SpeedI1\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedI2\", col(\"SpeedI2\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedFL\", col(\"SpeedFL\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedST\", col(\"SpeedST\").cast(IntegerType()))\n",
    "    .withColumn(\"IsPersonalBest\", col(\"IsPersonalBest\").cast(BooleanType()))\n",
    "    .withColumn(\"TyreLife\", col(\"TyreLife\").cast(IntegerType()))\n",
    "    .withColumn(\"FreshTyre\", col(\"FreshTyre\").cast(BooleanType()))\n",
    "    .withColumn(\"LapStartTime\", split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapStartDate\", to_timestamp(\"LapStartDate\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    .withColumn(\"TrackStatus\", col(\"TrackStatus\").cast(IntegerType()))\n",
    "    .withColumn(\"Position\", col(\"Position\").cast(IntegerType()))\n",
    "    .withColumn(\"Deleted\", col(\"Deleted\").cast(BooleanType()))\n",
    "    .withColumn(\"FastF1Generated\", col(\"FastF1Generated\").cast(BooleanType()))\n",
    "    .withColumn(\"IsAccurate\", col(\"IsAccurate\").cast(BooleanType()))\n",
    "    .withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
    "    .withColumn(\"LapSessionTime\", split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    ")\n",
    "\n",
    "lap_data = lap_data.drop(col(\"Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1fac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the result\n",
    "# lap_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ace846",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf73b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define windows\n",
    "start_position_window = Window.partitionBy(\"Year\", \"EventName\", \"Driver\")\n",
    "lap_order_window = start_position_window.orderBy(\"LapNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0524bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"rolling_avg_laptime\", avg(\"LapTime\").over(lap_order_window.rowsBetween(Window.unboundedPreceding, 0)))\n",
    "    .withColumn(\"pit_in_lap\", when(col(\"PitInTime\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"pit_exit_lap\", when(col(\"PitOutTime\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\n",
    "        \"last_pit_lap\",\n",
    "        coalesce(\n",
    "            max(\"pit_exit_lap\").over(lap_order_window.rowsBetween(Window.unboundedPreceding, 0)),\n",
    "            lit(0)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"laps_since_last_pit\", col(\"LapNumber\") - col(\"last_pit_lap\"))\n",
    "    .withColumn(\n",
    "        \"prev_compound\", \n",
    "        when(\n",
    "            col(\"LapNumber\") == 1, col(\"Compound\")\n",
    "        ).otherwise(\n",
    "            lag(\"Compound\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pit_stop_duration\",\n",
    "        when(\n",
    "            col(\"PitOutTime\").isNull(),\n",
    "            lit(0)\n",
    "        ).otherwise(\n",
    "            col(\"PitOutTime\") - lag(\"PitInTime\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"max_pit_stop_duration\", max(\"pit_stop_duration\").over(lap_order_window))\n",
    "    .withColumn(\"start_position\", first(when(col(\"LapNumber\") == 1, col(\"Position\")), ignorenulls=True).over(start_position_window))\n",
    "    .withColumn(\"position_change_since_race_start\", col(\"start_position\") - col(\"Position\"))\n",
    "    .withColumn(\n",
    "        \"fastest_sector\", when(\n",
    "            (col(\"Sector1Time\") <= col(\"Sector2Time\")) & (col(\"Sector1Time\") <= col(\"Sector3Time\")), 1\n",
    "        ).when(\n",
    "            (col(\"Sector2Time\") <= col(\"Sector1Time\")) & (col(\"Sector2Time\") <= col(\"Sector3Time\")), 2\n",
    "        ).otherwise(3)\n",
    "    )\n",
    ")\n",
    "\n",
    "lap_data = lap_data.drop(\"Sector1SessionTime\", \"Sector2SessionTime\", \"Sector3SessionTime\", \"DeletedReason\", \"IsAccurate\", \"LapStartDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48d5b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+--------+------------+-----------+--------+-------+---------------+----+--------------------+-------+--------------+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "|Driver|DriverNumber|LapTime|LapNumber|Stint|PitOutTime|PitInTime|Sector1Time|Sector2Time|Sector3Time|SpeedI1|SpeedI2|SpeedFL|SpeedST|IsPersonalBest|Compound|TyreLife|FreshTyre|    Team|LapStartTime|TrackStatus|Position|Deleted|FastF1Generated|Year|           EventName|Session|LapSessionTime|rolling_avg_laptime|pit_in_lap|pit_exit_lap|last_pit_lap|laps_since_last_pit|prev_compound|pit_stop_duration|max_pit_stop_duration|start_position|position_change_since_race_start|fastest_sector|\n",
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+--------+------------+-----------+--------+-------+---------------+----+--------------------+-------+--------------+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "|   ALB|          23|101.738|        1|    1|      NULL|     NULL|       NULL|     40.201|     36.214|    291|    298|    212|    270|         false|  MEDIUM|       1|     true|Williams|    3730.161|          1|      17|  false|          false|2022|Abu Dhabi Grand Prix|      R|      3832.133|            101.738|         0|           0|           0|                  1|       MEDIUM|              0.0|                  0.0|            17|                               0|             3|\n",
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+--------+------------+-----------+--------+-------+---------------+----+--------------------+-------+--------------+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lap_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd1d0c",
   "metadata": {},
   "source": [
    "### Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63af09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the file name from the file path\n",
    "file_name_col = input_file_name()\n",
    "\n",
    "# Extract the event name and session from the file name\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"Year\", regexp_extract(file_name_col, r\"/(\\d{4})_[^/]+_[QR]\\.csv$\", 1))\n",
    "    .withColumn(\"EventName\", regexp_replace(regexp_extract(file_name_col, r\"/\\d{4}_(.+)_[QR]\\.csv$\", 1), \"_\", \" \"))\n",
    "    .withColumn(\"Session\", regexp_extract(file_name_col, r\"/\\d{4}_[^/]+_([QR])\\.csv$\", 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f14c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql view\n",
    "telemetry_data.createOrReplaceTempView(\"telemetry\")\n",
    "\n",
    "# Filter for only Race events\n",
    "telemetry_data = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM telemetry\n",
    "    WHERE Session = 'R'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451a441",
   "metadata": {},
   "source": [
    "**Fixing Datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2936752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check datatypes\n",
    "# telemetry_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0660d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"Date\", to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    .withColumn(\"RPM\", col(\"RPM\").cast(IntegerType()))\n",
    "    .withColumn(\"Speed\", col(\"Speed\").cast(IntegerType()))\n",
    "    .withColumn(\"nGear\", col(\"nGear\").cast(IntegerType()))\n",
    "    .withColumn(\"Throttle\", col(\"Throttle\").cast(IntegerType()))\n",
    "    .withColumn(\"Brake\", col(\"Brake\").cast(BooleanType()).cast(IntegerType()))\n",
    "    .withColumn(\"DRS\", col(\"DRS\").cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"DataCollectionTime\", split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"SessionTime\", split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Distance\", col(\"Distance\").cast(FloatType()))\n",
    "    .withColumn(\"LapNumber\", col(\"LapNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"IsDRSActive\", when(\n",
    "            col(\"DRS\").isin(10, 12, 14), 1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "telemetry_data = telemetry_data.drop(col(\"Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095adaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-----+--------+-----+---+------+-----------+--------+------+---------+----+----------------+-------+------------------+-----------+\n",
      "|                Date|  RPM|Speed|nGear|Throttle|Brake|DRS|Source|SessionTime|Distance|Driver|LapNumber|Year|       EventName|Session|DataCollectionTime|IsDRSActive|\n",
      "+--------------------+-----+-----+-----+--------+-----+---+------+-----------+--------+------+---------+----+----------------+-------+------------------+-----------+\n",
      "|2023-08-27 13:03:...|10093|    0|    1|      15|    0|  1|   car|   3725.042|     0.0|   VER|        1|2023|Dutch Grand Prix|      R|             0.082|          0|\n",
      "+--------------------+-----+-----+-----+--------+-----+---+------+-----------+--------+------+---------+----+----------------+-------+------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Show the result\n",
    "telemetry_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f788ef",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d332796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window\n",
    "window_spec = Window.partitionBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").orderBy(\"SessionTime\")\n",
    "last_50_window = window_spec.rowsBetween(-49, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0f24cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-lap aggregates\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"avg_speed_last_lap\", avg(\"Speed\").over(window_spec))\n",
    "    .withColumn(\"max_speed_last_lap\", max(\"Speed\").over(window_spec))\n",
    "    .withColumn(\"avg_throttle_last_lap\", avg(\"Throttle\").over(window_spec))\n",
    "    .withColumn(\"avg_brake_last_lap\", avg(\"Brake\").over(window_spec))\n",
    "    .withColumn(\"avg_rpm\", avg(\"RPM\").over(window_spec))\n",
    "    .withColumn(\"gear_change\", when(col(\"nGear\") != lag(\"nGear\").over(window_spec), 1).otherwise(0))\n",
    "    .withColumn(\"gear_change_count\", sum(\"gear_change\").over(window_spec))\n",
    "    .withColumn(\n",
    "        \"DRS_activation_count\",\n",
    "        sum(\n",
    "            when(\n",
    "                (~lag(\"DRS\").over(window_spec).isin(10, 12, 14)) & (col(\"DRS\").isin(10, 12, 14)),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2fb68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling features over last 50 telemetry rows\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"rolling_throttle_mean\", avg(\"Throttle\").over(last_50_window))\n",
    "    .withColumn(\"rolling_brake_intensity\", avg(\"Brake\").over(last_50_window))\n",
    "    .withColumn(\"rolling_gear_change\", when(col(\"nGear\") != lag(\"nGear\").over(window_spec), 1).otherwise(0))\n",
    "    .withColumn(\"rolling_gear_change_rate\", avg(\"rolling_gear_change\").over(last_50_window))\n",
    "    .withColumn(\"rolling_speed_mean\", avg(\"Speed\").over(last_50_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9288123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sector features (define final 5% of distance per lap)\n",
    "max_distance = telemetry_data.groupBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").agg(max(\"Distance\").alias(\"max_dist\"))\n",
    "telemetry_data = telemetry_data.join(max_distance, on=[\"Year\", \"EventName\", \"Driver\", \"LapNumber\"])\n",
    "telemetry_data = telemetry_data.withColumn(\"in_final_sector\", col(\"Distance\") >= col(\"max_dist\") * 0.95)\n",
    "\n",
    "# Define new window\n",
    "final_sector_window = Window.partitionBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"final_sector_avg_speed\", avg(when(col(\"in_final_sector\"), col(\"Speed\"))).over(final_sector_window))\n",
    "    .withColumn(\"final_sector_throttle\", avg(when(col(\"in_final_sector\"), col(\"Throttle\"))).over(final_sector_window))\n",
    "    .withColumn(\"final_sector_brake\", avg(when(col(\"in_final_sector\"), col(\"Brake\"))).over(final_sector_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "094b2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telemetry_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d5ac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final per-lap features\n",
    "lap_feature_cols = [\n",
    "    \"EventName\", \"Driver\", \"LapNumber\", \"Year\", \"Session\",\n",
    "    \"avg_speed_last_lap\", \"max_speed_last_lap\",\n",
    "    \"avg_throttle_last_lap\", \"avg_brake_last_lap\",\n",
    "    \"gear_change_count\", \"avg_rpm\",\n",
    "    \"rolling_throttle_mean\", \"rolling_brake_intensity\",\n",
    "    \"rolling_gear_change_rate\", \"rolling_speed_mean\",\n",
    "    \"final_sector_avg_speed\", \"final_sector_throttle\", \n",
    "    \"final_sector_brake\"\n",
    "]\n",
    "\n",
    "# For all columns, take the FIRST value per (Driver, LapNumber)\n",
    "# Because window functions already populated each row with the same value within each lap\n",
    "aggregated_laps = (\n",
    "    telemetry_data\n",
    "    .select(*lap_feature_cols)\n",
    "    .groupBy(\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\")\n",
    "    .agg(*[\n",
    "        first(col_name).alias(col_name) \n",
    "        if col_name != \"DRS_activation_count\" \n",
    "        else last(col_name).alias(col_name) \n",
    "        for col_name in lap_feature_cols \n",
    "        if col_name not in (\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4892b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_laps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c6a39",
   "metadata": {},
   "source": [
    "### Joining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join lap_data and telemetry_data\n",
    "data = (\n",
    "    lap_data\n",
    "    .alias('lap')\n",
    "    .join(\n",
    "        aggregated_laps.alias('telemetry')\n",
    "        ,on=[\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\"]\n",
    "        ,how=\"outer\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec477f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"WillPitNextLap\", when(\n",
    "            lead(\"PitInTime\", 1).over(Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\").orderBy(\"LapNumber\")).isNotNull(), 1\n",
    "        )\n",
    "    .otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# data = data.drop(\"PitInTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65edc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- EventName: string (nullable = true)\n",
      " |-- Session: string (nullable = true)\n",
      " |-- Driver: string (nullable = true)\n",
      " |-- LapNumber: integer (nullable = true)\n",
      " |-- DriverNumber: integer (nullable = true)\n",
      " |-- LapTime: double (nullable = true)\n",
      " |-- Stint: integer (nullable = true)\n",
      " |-- PitOutTime: double (nullable = true)\n",
      " |-- PitInTime: double (nullable = true)\n",
      " |-- Sector1Time: double (nullable = true)\n",
      " |-- Sector2Time: double (nullable = true)\n",
      " |-- Sector3Time: double (nullable = true)\n",
      " |-- SpeedI1: integer (nullable = true)\n",
      " |-- SpeedI2: integer (nullable = true)\n",
      " |-- SpeedFL: integer (nullable = true)\n",
      " |-- SpeedST: integer (nullable = true)\n",
      " |-- IsPersonalBest: boolean (nullable = true)\n",
      " |-- Compound: string (nullable = true)\n",
      " |-- TyreLife: integer (nullable = true)\n",
      " |-- FreshTyre: boolean (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- LapStartTime: double (nullable = true)\n",
      " |-- TrackStatus: integer (nullable = true)\n",
      " |-- Position: integer (nullable = true)\n",
      " |-- Deleted: boolean (nullable = true)\n",
      " |-- FastF1Generated: boolean (nullable = true)\n",
      " |-- LapSessionTime: double (nullable = true)\n",
      " |-- rolling_avg_laptime: double (nullable = true)\n",
      " |-- pit_in_lap: integer (nullable = true)\n",
      " |-- pit_exit_lap: integer (nullable = true)\n",
      " |-- last_pit_lap: integer (nullable = true)\n",
      " |-- laps_since_last_pit: integer (nullable = true)\n",
      " |-- prev_compound: string (nullable = true)\n",
      " |-- pit_stop_duration: double (nullable = true)\n",
      " |-- max_pit_stop_duration: double (nullable = true)\n",
      " |-- start_position: integer (nullable = true)\n",
      " |-- position_change_since_race_start: integer (nullable = true)\n",
      " |-- fastest_sector: integer (nullable = true)\n",
      " |-- avg_speed_last_lap: double (nullable = true)\n",
      " |-- max_speed_last_lap: integer (nullable = true)\n",
      " |-- avg_throttle_last_lap: double (nullable = true)\n",
      " |-- avg_brake_last_lap: double (nullable = true)\n",
      " |-- gear_change_count: long (nullable = true)\n",
      " |-- avg_rpm: double (nullable = true)\n",
      " |-- rolling_throttle_mean: double (nullable = true)\n",
      " |-- rolling_brake_intensity: double (nullable = true)\n",
      " |-- rolling_gear_change_rate: double (nullable = true)\n",
      " |-- rolling_speed_mean: double (nullable = true)\n",
      " |-- final_sector_avg_speed: double (nullable = true)\n",
      " |-- final_sector_throttle: double (nullable = true)\n",
      " |-- final_sector_brake: double (nullable = true)\n",
      " |-- WillPitNextLap: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fde46b",
   "metadata": {},
   "source": [
    "**Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute null counts\n",
    "# null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# # Convert to a Row to filter in Python\n",
    "# null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# # Filter and print only columns with nulls\n",
    "# for col_name, count in null_counts_dict.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6faa6f1",
   "metadata": {},
   "source": [
    "LapTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab15d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check rows with missing values\n",
    "# data.filter(col(\"LapTime\").isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d06d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with NULL LapTime and Sector<1, 2, 3>Time are DNF so we drop these rows\n",
    "data = (\n",
    "    data\n",
    "    .filter(~(\n",
    "        col(\"Sector1Time\").isNull() & \n",
    "        col(\"Sector2Time\").isNull() & \n",
    "        col(\"Sector3Time\").isNull() & \n",
    "        col(\"LapTime\").isNull()\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove SectorTime columns\n",
    "data = data.drop(\"Sector1Time\", \"Sector2Time\", \"Sector3Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067cc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values - compute by subtracting the time at the end and at the start of the lap\n",
    "data = data.withColumn(\"LapTime\", col(\"LapSessionTime\") - col(\"LapStartTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdfc1f",
   "metadata": {},
   "source": [
    "Missing values recheck, since DNF rows were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute null counts\n",
    "# null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# # Convert to a Row to filter in Python\n",
    "# null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# # Filter and print only columns with nulls\n",
    "# for col_name, count in null_counts_dict.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836904c",
   "metadata": {},
   "source": [
    "SpeedI1, SpeedI2, SpeedFL, SpeedST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66719c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - speed rolling average\n",
    "driver_lap_window = Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\").orderBy(\"LapNumber\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# List of columns to process\n",
    "speed_cols = [\"SpeedI1\", \"SpeedI2\", \"SpeedFL\", \"SpeedST\"]\n",
    "\n",
    "# Fill missing values\n",
    "for col_name in speed_cols:\n",
    "    cumulative_avg = avg(col(col_name)).over(driver_lap_window)\n",
    "    data = (\n",
    "        data\n",
    "        .withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull(), cumulative_avg).otherwise(col(col_name))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - teammate's speed in same lap\n",
    "\n",
    "# Self-join on teammate info\n",
    "teammate_join = data.alias(\"self\").join(\n",
    "    data.alias(\"tm\"),\n",
    "    on=[\n",
    "        col(\"self.Year\") == col(\"tm.Year\"),\n",
    "        col(\"self.EventName\") == col(\"tm.EventName\"),\n",
    "        col(\"self.Session\") == col(\"tm.Session\"),\n",
    "        col(\"self.Team\") == col(\"tm.Team\"),\n",
    "        col(\"self.LapNumber\") == col(\"tm.LapNumber\"),\n",
    "        col(\"self.Driver\") != col(\"tm.Driver\")\n",
    "    ],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Replace missing values from teammate values\n",
    "updated_cols = [\n",
    "    coalesce(col(f\"self.{col_name}\"), col(f\"tm.{col_name}\")).alias(col_name)\n",
    "    if col_name in speed_cols else col(f\"self.{col_name}\")\n",
    "    for col_name in data.columns\n",
    "]\n",
    "\n",
    "# Assign back to `data` (replacing the original one)\n",
    "data = teammate_join.select(*updated_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - finish line speed with longest straight speed\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"SpeedFL\",\n",
    "        when(col(\"SpeedFL\").isNull(), col(\"SpeedST\")).otherwise(col(\"SpeedFL\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"SpeedI1\").isNull() |\n",
    "#     col(\"SpeedI2\").isNull() |\n",
    "#     col(\"SpeedFL\").isNull() |\n",
    "#     col(\"SpeedST\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0246f",
   "metadata": {},
   "source": [
    "pit_stop_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1093e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"pit_stop_duration\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7abdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"pit_stop_duration\").isNull()\n",
    "# ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db32852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values seem to be mistakes, so we set PitOutTime to NULL and recompute pit_stop_duration and max_pit_stop_duration\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"PitOutTime\",\n",
    "        when(col(\"pit_stop_duration\").isNull(), None).otherwise(col(\"PitOutTime\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pit_stop_duration\",\n",
    "        when(\n",
    "            col(\"PitOutTime\").isNull(),\n",
    "            lit(0)\n",
    "        ).otherwise(\n",
    "            col(\"PitOutTime\") - lag(\"PitInTime\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"max_pit_stop_duration\", max(\"pit_stop_duration\").over(lap_order_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck\n",
    "# data.filter(\n",
    "#     col(\"pit_stop_duration\").isNull()\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43db11",
   "metadata": {},
   "source": [
    "max_pit_stop_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca96008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values\n",
    "# data.filter(\n",
    "#     col(\"max_pit_stop_duration\").isNull()\n",
    "# ).count()\n",
    "\n",
    "# # No more :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute null counts\n",
    "# null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# # Convert to a Row to filter in Python\n",
    "# null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# # Filter and print only columns with nulls\n",
    "# for col_name, count in null_counts_dict.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dd75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef11b74",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8c5e87e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Get total number of rows\n",
    "total_rows = data.count()\n",
    "\n",
    "# Create an expression for each column to count nulls\n",
    "missing_exprs = [\n",
    "    (count(when(col(c).isNull(), c)) / total_rows).alias(c)\n",
    "    for c in data.columns\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the % of missing values per column\n",
    "missing_percent_df = data.select(missing_exprs)\n",
    "\n",
    "# Show the result\n",
    "missing_percent_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"PitOutTime\", \"PitInTime\", \"Session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42778efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_data = data.filter(~((col(\"EventName\") == \"Abu Dhabi Grand Prix\") & (col(\"Year\") == 2023)) & ~((col(\"EventName\") == \"Las Vegas Grand Prix\") & (col(\"Year\") == 2023)))\n",
    "val_data = data.filter((col(\"EventName\") == \"Las Vegas Grand Prix\") & (col(\"Year\") == 2023))\n",
    "test_data = data.filter((col(\"EventName\") == \"Abu Dhabi Grand Prix\") & (col(\"Year\") == 2023))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3f8bfd8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import optuna\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, ChiSqSelector, OneHotEncoder\n",
    "from pyspark.sql.functions import col\n",
    "import uuid\n",
    "\n",
    "def train_model(target, train_data, val_data, test_data, model_type, optimize, num_features=20, n_trials=2):\n",
    "    \"\"\"\n",
    "    Train a model with optional Optuna hyperparameter optimization, feature selection, and scaling for LR/MLP.\n",
    "    \n",
    "    Args:\n",
    "        target (str): Target column name.\n",
    "        train_data: PySpark DataFrame for training.\n",
    "        val_data: PySpark DataFrame for validation during optimization.\n",
    "        test_data: PySpark DataFrame for final evaluation.\n",
    "        model_type (str): Model type ('xgb', 'rf', 'lr', 'mlp').\n",
    "        optimize (bool): If True, optimize hyperparameters with Optuna; if False, use defaults.\n",
    "        num_features (int): Number of features to select using Chi-Square Selector.\n",
    "        n_trials (int): Number of Optuna trials if optimize=True.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained model, areaUnderPR score on test data)\n",
    "    \"\"\"\n",
    "    # Validate model_type\n",
    "    valid_models = ['xgb', 'rf', 'lr', 'mlp']\n",
    "    if model_type not in valid_models:\n",
    "        raise ValueError(f\"model_type must be one of {valid_models}\")\n",
    "\n",
    "# Define indexers and encoders for the relevant columns\n",
    "    indexers_and_encoders = [\n",
    "    # Indexers\n",
    "    StringIndexer(inputCol=\"Team\", outputCol=\"TeamIndex\"),\n",
    "    StringIndexer(inputCol=\"Compound\", outputCol=\"CompoundIndex\"),\n",
    "    StringIndexer(inputCol=\"Driver\", outputCol=\"DriverIndex\"),\n",
    "    StringIndexer(inputCol=\"EventName\", outputCol=\"EventNameIndex\"),\n",
    "    StringIndexer(inputCol=\"prev_compound\", outputCol=\"prev_compound_index\"),\n",
    "    \n",
    "    # OneHot Encoders\n",
    "    OneHotEncoder(inputCol=\"CompoundIndex\", outputCol=\"CompoundIndex_ohe\"),\n",
    "    OneHotEncoder(inputCol=\"prev_compound_index\", outputCol=\"prev_compound_ohe\")\n",
    "    ]\n",
    "\n",
    "    # Cache data\n",
    "    train_data.cache()\n",
    "    val_data.cache()\n",
    "    test_data.cache()\n",
    "\n",
    "    # Apply indexers_and_encoders to all datasets (fit on train_data only)\n",
    "    indexer_pipeline = Pipeline(stages=indexers_and_encoders)\n",
    "    indexer_model = indexer_pipeline.fit(train_data)\n",
    "    train_data = indexer_model.transform(train_data)\n",
    "    val_data = indexer_model.transform(val_data)\n",
    "    test_data = indexer_model.transform(test_data)\n",
    "\n",
    "    # Get all columns for feature selection (exclude target, raw categorical columns)\n",
    "    categorical_cols = [\"Team\", \"Compound\", \"Driver\", \"EventName\", \"Session\", \"prev_compound\", \"CompoundIndex\", \"prev_compound_index\"]\n",
    "    feature_cols = [col for col in train_data.columns if col != target and col not in categorical_cols]\n",
    "    \n",
    "    # Temporary assembler for feature selection\n",
    "    temp_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    temp_pipeline = Pipeline(stages=[temp_assembler])\n",
    "    train_data = temp_pipeline.fit(train_data).transform(train_data)\n",
    "\n",
    "    # Apply Chi-Square Selector\n",
    "    selector = ChiSqSelector(\n",
    "        numTopFeatures=num_features,\n",
    "        featuresCol=\"features\",\n",
    "        outputCol=\"selected_features\",\n",
    "        labelCol=target\n",
    "    )\n",
    "    selector_model = selector.fit(train_data)\n",
    "    selected_indices = selector_model.selectedFeatures\n",
    "    selected_feature_names = [feature_cols[i] for i in selected_indices]\n",
    "    print(f\"Selected {len(selected_feature_names)} features: {selected_feature_names}\")\n",
    "\n",
    "    # Create new assembler with selected features\n",
    "    assembler = VectorAssembler(inputCols=selected_feature_names, outputCol=\"features\")\n",
    "    feature_pipeline = Pipeline(stages=[assembler])\n",
    "\n",
    "    # Update datasets with selected features\n",
    "    train_data = feature_pipeline.fit(train_data).transform(train_data)\n",
    "    val_data = feature_pipeline.fit(val_data).transform(val_data)\n",
    "    test_data = feature_pipeline.fit(test_data).transform(test_data)\n",
    "\n",
    "    def get_classifier(trial=None):\n",
    "        \"\"\"Define classifier based on model_type and optional trial parameters.\"\"\"\n",
    "        if model_type == 'rf':\n",
    "            if optimize and trial:\n",
    "                num_trees = trial.suggest_int(\"numTrees\", 10, 100)\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 5, 30)\n",
    "                min_instances_per_node = trial.suggest_int(\"minInstancesPerNode\", 1, 10)\n",
    "                subsampling_rate = trial.suggest_float(\"subsamplingRate\", 0.5, 1.0)\n",
    "                max_bins = trial.suggest_int(\"maxBins\", 10, 50)\n",
    "                return RandomForestClassifier(labelCol=target, featuresCol=\"features\",\n",
    "                                            numTrees=num_trees, maxDepth=max_depth,\n",
    "                                            minInstancesPerNode=min_instances_per_node,\n",
    "                                            subsamplingRate=subsampling_rate, maxBins=max_bins)\n",
    "            return RandomForestClassifier(labelCol=target, featuresCol=\"features\")\n",
    "        \n",
    "        elif model_type == 'xgb':\n",
    "            if optimize and trial:\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 3, 10)\n",
    "                num_round = trial.suggest_int(\"num_round\", 10, 100)\n",
    "                eta = trial.suggest_float(\"eta\", 0.01, 0.3)\n",
    "                subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "                colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "                return SparkXGBClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"features\",\n",
    "                    max_depth=max_depth,\n",
    "                    num_round=num_round,\n",
    "                    eta=eta,\n",
    "                    subsample=subsample,\n",
    "                    colsample_bytree=colsample_bytree\n",
    "                )\n",
    "            return SparkXGBClassifier(labelCol=target, featuresCol=\"features\")\n",
    "        \n",
    "        elif model_type == 'lr':\n",
    "            if optimize and trial:\n",
    "                reg_param = trial.suggest_float(\"regParam\", 1e-5, 0.5, log=True)\n",
    "                elastic_net = trial.suggest_float(\"elasticNetParam\", 0.0, 1.0)\n",
    "                tol = trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 300)\n",
    "                fit_intercept = trial.suggest_categorical(\"fitIntercept\", [True, False])\n",
    "                return LogisticRegression(labelCol=target, featuresCol=\"scaled_features\",\n",
    "                                        regParam=reg_param, elasticNetParam=elastic_net,\n",
    "                                        tol=tol, maxIter=max_iter, fitIntercept=fit_intercept)\n",
    "            return LogisticRegression(labelCol=target, featuresCol=\"scaled_features\")\n",
    "        \n",
    "        elif model_type == 'mlp':\n",
    "            layers = [train_data.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"], 64, 32, 2]  \n",
    "            if optimize and trial:\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 200)\n",
    "                block_size = trial.suggest_int(\"blockSize\", 32, 128)\n",
    "                step_size = trial.suggest_float(\"stepSize\", 0.001, 0.1, log=True)\n",
    "                return MultilayerPerceptronClassifier(labelCol=target, featuresCol=\"scaled_features\",\n",
    "                                                    layers=layers, maxIter=max_iter, blockSize=block_size,\n",
    "                                                    stepSize=step_size)\n",
    "            return MultilayerPerceptronClassifier(labelCol=target, featuresCol=\"scaled_features\",\n",
    "                                                layers=layers)\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Objective function for Optuna optimization using validation data.\"\"\"\n",
    "        classifier = get_classifier(trial)\n",
    "        # Add scaler for lr/mlp\n",
    "        stages = [assembler]\n",
    "        if model_type in ['lr', 'mlp']:\n",
    "            scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "            stages.append(scaler)\n",
    "        stages.append(classifier)\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        try:\n",
    "            model = pipeline.fit(train_data)\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "            auc = evaluator.evaluate(model.transform(val_data))\n",
    "            return auc\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Train model\n",
    "    if optimize:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\", \n",
    "            study_name=f\"{model_type}_optimization_{uuid.uuid4()}\"\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
    "        print(f\"Best areaUnderPR: {study.best_value}\")\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        classifier = get_classifier(study.best_trial)\n",
    "    else:\n",
    "        classifier = get_classifier()\n",
    "\n",
    "    # Build pipeline\n",
    "    stages = [assembler]\n",
    "    if model_type in ['lr', 'mlp']:\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        stages.append(scaler)\n",
    "    stages.append(classifier)\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    # Fit and evaluate on test data\n",
    "    model = pipeline.fit(train_data)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "    auc = evaluator.evaluate(model.transform(test_data))\n",
    "    \n",
    "    # Unpersist data\n",
    "    train_data.unpersist()\n",
    "    val_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "    \n",
    "    return model, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col\n",
    "import uuid\n",
    "import numpy as np\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def expand_one_hot_vectors(df, ohe_columns):\n",
    "    \"\"\"Expand one-hot encoded vector columns into individual binary columns.\"\"\"\n",
    "    for vec_col in ohe_columns:\n",
    "        # Convert vector to array\n",
    "        df = df.withColumn(f\"{vec_col}_array\", vector_to_array(col(vec_col)))\n",
    "        \n",
    "        # Get size of the vector (number of categories)\n",
    "        size = len(df.select(vec_col).first()[0])\n",
    "        \n",
    "        # Create individual columns for each category\n",
    "        for i in range(size):\n",
    "            df = df.withColumn(f\"{vec_col}_{i}\", col(f\"{vec_col}_array\")[i])\n",
    "    \n",
    "    return df.drop(*[f\"{col}_array\" for col in ohe_columns])\n",
    "\n",
    "def train_model(target, train_data, val_data, test_data, model_type, optimize, num_features=20, n_trials=2):\n",
    "    \"\"\"\n",
    "    Train a model with optional Optuna hyperparameter optimization and L1-based feature selection for LR/MLP.\n",
    "    \n",
    "    Args:\n",
    "        target (str): Target column name.\n",
    "        train_data: PySpark DataFrame for training.\n",
    "        val_data: PySpark DataFrame for validation during optimization.\n",
    "        test_data: PySpark DataFrame for final evaluation.\n",
    "        model_type (str): Model type ('xgb', 'rf', 'lr', 'mlp').\n",
    "        optimize (bool): If True, optimize hyperparameters with Optuna; if False, use defaults.\n",
    "        num_features (int): Number of features to select for LR/MLP using L1 regularization.\n",
    "        n_trials (int): Number of Optuna trials if optimize=True.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained model, areaUnderPR score on test data)\n",
    "    \"\"\"\n",
    "    # Validate model_type\n",
    "    valid_models = ['xgb', 'rf', 'lr', 'mlp']\n",
    "    if model_type not in valid_models:\n",
    "        raise ValueError(f\"model_type must be one of {valid_models}\")\n",
    "\n",
    "    # Define indexers and encoders for the relevant columns\n",
    "    indexers_and_encoders = [\n",
    "        # Indexers\n",
    "        StringIndexer(inputCol=\"Team\", outputCol=\"TeamIndex\"),\n",
    "        StringIndexer(inputCol=\"Compound\", outputCol=\"CompoundIndex\"),\n",
    "        StringIndexer(inputCol=\"Driver\", outputCol=\"DriverIndex\"),\n",
    "        StringIndexer(inputCol=\"EventName\", outputCol=\"EventNameIndex\"),\n",
    "        StringIndexer(inputCol=\"prev_compound\", outputCol=\"prev_compound_index\"),\n",
    "        # OneHot Encoders\n",
    "        OneHotEncoder(inputCol=\"CompoundIndex\", outputCol=\"CompoundIndex_ohe\"),\n",
    "        OneHotEncoder(inputCol=\"prev_compound_index\", outputCol=\"prev_compound_ohe\")\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Cache data\n",
    "    train_data.cache()\n",
    "    val_data.cache()\n",
    "    test_data.cache()\n",
    "\n",
    "    # Apply indexers_and_encoders to all datasets (fit on train_data only)\n",
    "    indexer_pipeline = Pipeline(stages=indexers_and_encoders)\n",
    "    indexer_model = indexer_pipeline.fit(train_data)\n",
    "    train_data = indexer_model.transform(train_data)\n",
    "    val_data = indexer_model.transform(val_data)\n",
    "    test_data = indexer_model.transform(test_data)\n",
    "\n",
    "    # Get all columns for feature selection (exclude target, raw categorical columns)\n",
    "    categorical_cols = [\"Team\", \"Compound\", \"Driver\", \"EventName\", \"prev_compound\", \"CompoundIndex\", \"prev_compound_index\"]\n",
    "    feature_cols = [col for col in train_data.columns if col != target and col not in categorical_cols]\n",
    "\n",
    "    # Perform L1-based feature selection for lr and mlp\n",
    "    selected_feature_names = feature_cols  # Default: use all features\n",
    "    if model_type in ['lr', 'mlp']:\n",
    "        # First expand one-hot encoded vectors\n",
    "        ohe_columns = [\"CompoundIndex_ohe\", \"prev_compound_ohe\"]  # Add any other OHE columns\n",
    "        train_expanded = expand_one_hot_vectors(train_data, ohe_columns)\n",
    "        val_expanded = expand_one_hot_vectors(val_data, ohe_columns)\n",
    "        test_expanded = expand_one_hot_vectors(test_data, ohe_columns)\n",
    "        \n",
    "        # Get all feature columns (now including expanded binary columns)\n",
    "        feature_cols = [col for col in train_expanded.columns \n",
    "                       if col != target \n",
    "                       and col not in categorical_cols \n",
    "                       and not col.endswith(\"_array\")]\n",
    "        \n",
    "        # Create temporary assembler for all features\n",
    "        temp_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"temp_l1_features\")\n",
    "        temp_scaler = StandardScaler(inputCol=\"temp_l1_features\", outputCol=\"temp_scaled_features\", withMean=True, withStd=True)\n",
    "        \n",
    "        # Fit an L1-regularized LogisticRegression\n",
    "        l1_lr = LogisticRegression(\n",
    "            labelCol=target,\n",
    "            featuresCol=\"temp_scaled_features\",\n",
    "            regParam=0.1,\n",
    "            elasticNetParam=1.0,\n",
    "            maxIter=100\n",
    "        )\n",
    "        l1_pipeline = Pipeline(stages=[temp_assembler, temp_scaler, l1_lr])\n",
    "        l1_model = l1_pipeline.fit(train_expanded)\n",
    "        \n",
    "        # Extract coefficients\n",
    "        coefficients = np.abs(l1_model.stages[-1].coefficients.toArray())\n",
    "        \n",
    "        # Safely select top features\n",
    "        non_zero_indices = np.where(coefficients > 0)[0]\n",
    "        if len(non_zero_indices) > 0:\n",
    "            top_n = min(num_features, len(non_zero_indices))\n",
    "            top_indices = np.argsort(coefficients[non_zero_indices])[::-1][:top_n]\n",
    "            selected_feature_names = [feature_cols[non_zero_indices[i]] for i in top_indices]\n",
    "        else:\n",
    "            selected_feature_names = feature_cols[:num_features]\n",
    "            \n",
    "        print(f\"Selected {len(selected_feature_names)} features for {model_type}: {selected_feature_names}\")\n",
    "        \n",
    "        # Clean up temporary columns after feature selection\n",
    "        train_data = train_expanded.drop(\"temp_l1_features\", \"temp_scaled_features\")\n",
    "        val_data = val_expanded.drop(\"temp_l1_features\", \"temp_scaled_features\")\n",
    "        test_data = test_expanded.drop(\"temp_l1_features\", \"temp_scaled_features\")\n",
    "\n",
    "    # Create assembler with selected features\n",
    "    assembler = VectorAssembler(inputCols=selected_feature_names, outputCol=\"features\")\n",
    "    feature_pipeline = Pipeline(stages=[assembler])\n",
    "\n",
    "    # Update datasets with selected features\n",
    "    train_data = feature_pipeline.fit(train_data).transform(train_data)\n",
    "    val_data = feature_pipeline.fit(val_data).transform(val_data)\n",
    "    test_data = feature_pipeline.fit(test_data).transform(test_data)\n",
    "\n",
    "    def get_classifier(trial=None):\n",
    "        \"\"\"Define classifier based on model_type and optional trial parameters.\"\"\"\n",
    "        if model_type == 'rf':\n",
    "            if optimize and trial:\n",
    "                num_trees = trial.suggest_int(\"numTrees\", 10, 100)\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 5, 30)\n",
    "                min_instances_per_node = trial.suggest_int(\"minInstancesPerNode\", 1, 10)\n",
    "                subsampling_rate = trial.suggest_float(\"subsamplingRate\", 0.5, 1.0)\n",
    "                max_bins = trial.suggest_int(\"maxBins\", 10, 50)\n",
    "                return RandomForestClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"features\",\n",
    "                    numTrees=num_trees,\n",
    "                    maxDepth=max_depth,\n",
    "                    minInstancesPerNode=min_instances_per_node,\n",
    "                    subsamplingRate=subsampling_rate,\n",
    "                    maxBins=max_bins\n",
    "                )\n",
    "            return RandomForestClassifier(labelCol=target, featuresCol=\"features\")\n",
    "        \n",
    "        elif model_type == 'xgb':\n",
    "            if optimize and trial:\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 3, 10)\n",
    "                num_round = trial.suggest_int(\"num_round\", 10, 100)\n",
    "                eta = trial.suggest_float(\"eta\", 0.01, 0.3)\n",
    "                subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "                colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "                return SparkXGBClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"features\",\n",
    "                    max_depth=max_depth,\n",
    "                    num_round=num_round,\n",
    "                    eta=eta,\n",
    "                    subsample=subsample,\n",
    "                    colsample_bytree=colsample_bytree\n",
    "                )\n",
    "            return SparkXGBClassifier(labelCol=target, featuresCol=\"features\")\n",
    "        \n",
    "        elif model_type == 'lr':\n",
    "            if optimize and trial:\n",
    "                reg_param = trial.suggest_float(\"regParam\", 1e-5, 0.5, log=True)\n",
    "                elastic_net = trial.suggest_float(\"elasticNetParam\", 0.0, 1.0)\n",
    "                tol = trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 300)\n",
    "                fit_intercept = trial.suggest_categorical(\"fitIntercept\", [True, False])\n",
    "                return LogisticRegression(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    regParam=reg_param,\n",
    "                    elasticNetParam=elastic_net,\n",
    "                    tol=tol,\n",
    "                    maxIter=max_iter,\n",
    "                    fitIntercept=fit_intercept\n",
    "                )\n",
    "            return LogisticRegression(labelCol=target, featuresCol=\"scaled_features\")\n",
    "        \n",
    "        elif model_type == 'mlp':\n",
    "            layers = [train_data.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"], 64, 32, 2]\n",
    "            if optimize and trial:\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 200)\n",
    "                block_size = trial.suggest_int(\"blockSize\", 32, 128)\n",
    "                step_size = trial.suggest_float(\"stepSize\", 0.001, 0.1, log=True)\n",
    "                return MultilayerPerceptronClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    layers=layers,\n",
    "                    maxIter=max_iter,\n",
    "                    blockSize=block_size,\n",
    "                    stepSize=step_size\n",
    "                )\n",
    "            return MultilayerPerceptronClassifier(\n",
    "                labelCol=target,\n",
    "                featuresCol=\"scaled_features\",\n",
    "                layers=layers\n",
    "            )\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Objective function for Optuna\"\"\"\n",
    "        classifier = get_classifier(trial)\n",
    "        stages = []\n",
    "        \n",
    "        # Add feature assembler (only if features column doesn't exist)\n",
    "        if \"features\" not in train_data.columns:\n",
    "            stages.append(assembler)\n",
    "\n",
    "        # Add scaler for lr/mlp\n",
    "        if model_type in ['lr', 'mlp']:\n",
    "            stages.append(StandardScaler(\n",
    "                inputCol=\"features\",\n",
    "                outputCol=\"scaled_features\",\n",
    "                withMean=True,\n",
    "                withStd=True\n",
    "            ))\n",
    "        stages.append(classifier)\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        try:\n",
    "            model = pipeline.fit(train_data)\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "            auc = evaluator.evaluate(model.transform(val_data))\n",
    "            return auc\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Train model\n",
    "    if optimize:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"{model_type}_optimization_{uuid.uuid4()}\"\n",
    "        )\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
    "        print(f\"Best areaUnderPR: {study.best_value}\")\n",
    "        classifier = get_classifier(study.best_trial)\n",
    "    else:\n",
    "        classifier = get_classifier()\n",
    "\n",
    "    # Build pipeline\n",
    "    stages = [assembler]\n",
    "    if model_type in ['lr', 'mlp']:\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        stages.append(scaler)\n",
    "    stages.append(classifier)\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    for name, df in [('train', train_data), ('test', test_data)]:\n",
    "        for col_name in ['features', 'scaled_features']:\n",
    "            if col_name in df.columns:\n",
    "                df = df.drop(col_name)\n",
    "        if name == 'train':\n",
    "            train_data = df\n",
    "        else:\n",
    "            test_data = df\n",
    "    \n",
    "    # Fit and evaluate on test data\n",
    "    model = pipeline.fit(train_data)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "    auc = evaluator.evaluate(model.transform(test_data))\n",
    "    \n",
    "    # Unpersist data\n",
    "    train_data.unpersist()\n",
    "    val_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "    \n",
    "    return model, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee549e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 20 features for mlp: ['Year', 'LapNumber', 'DriverNumber', 'LapTime', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'IsPersonalBest', 'TyreLife', 'FreshTyre', 'LapStartTime', 'TrackStatus', 'Position', 'Deleted', 'FastF1Generated', 'LapSessionTime', 'rolling_avg_laptime', 'pit_in_lap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-16 08:28:30,339] A new study created in memory with name: mlp_optimization_866eb572-02bc-4bc3-92c9-523a641cd583\n",
      "[I 2025-05-16 08:31:38,795] Trial 0 finished with value: 0.07731311171860229 and parameters: {'maxIter': 138, 'blockSize': 72, 'stepSize': 0.005578731491637231}. Best is trial 0 with value: 0.07731311171860229.\n",
      "[I 2025-05-16 08:34:59,409] Trial 1 finished with value: 0.0571879934195168 and parameters: {'maxIter': 172, 'blockSize': 109, 'stepSize': 0.0035097974659530715}. Best is trial 0 with value: 0.07731311171860229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial for mlp: {'maxIter': 138, 'blockSize': 72, 'stepSize': 0.005578731491637231}\n",
      "Best areaUnderPR: 0.07731311171860229\n",
      "Model AUC on test data: 0.1381931347361728\n"
     ]
    }
   ],
   "source": [
    "model, auc = train_model(\n",
    "    target=\"WillPitNextLap\",\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    model_type=\"xgb\",\n",
    "    optimize=True,\n",
    "    num_features=333,\n",
    "    n_trials=50\n",
    ")\n",
    "print(f\"Model AUC on test data: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e1d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|WillPitNextLap|count|\n",
      "+--------------+-----+\n",
      "|             0|44169|\n",
      "|             1| 1642|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "train_data.groupBy(\"WillPitNextLap\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2791a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
