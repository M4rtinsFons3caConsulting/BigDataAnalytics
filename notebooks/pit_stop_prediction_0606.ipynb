{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333f740",
   "metadata": {},
   "source": [
    "# **F1 Pit Stop Prediction**\n",
    "\n",
    "This project was developed using the **FastF1** library data and aims to predict if the driver will be going to the pits in the next lap.<br>\n",
    "To collect the data, the team developed a `data_collection.py` script. This script allows the user to collect lap, telemetry and weather data.\n",
    "\n",
    "The metadata of the datasets used in this project can be found at the **FastF1** documentation page, at https://docs.fastf1.dev/core.html. (Visited on May 10th, 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387603e",
   "metadata": {},
   "source": [
    "## 0. Imports, Constants and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff6fea",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, regexp_replace, col, when, to_timestamp, lead, avg, lag, max, first, last, split, coalesce, lit, row_number, sum\n",
    "from pyspark.sql.types import IntegerType, BooleanType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ff075",
   "metadata": {},
   "source": [
    "**Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63853f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# Directories\n",
    "ROOT = Path.cwd().parent\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "# Colors\n",
    "BLUE = '#003773'\n",
    "RED = '#ED1E36'\n",
    "YELLOW = '#FABB23'\n",
    "TEAM_COLORS = {\n",
    "    'Red Bull Racing': BLUE\n",
    "    ,'Mercedes': '#00A19B'\n",
    "    ,'Ferrari': '#A6051A'\n",
    "    ,'McLaren': '#FF8700'\n",
    "    ,'Aston Martin': '#00665E'\n",
    "    ,'Alpine': '#FD4BC7'\n",
    "    ,'AlphaTauri': '#00293F'\n",
    "    ,'Alfa Romeo': '#972738'\n",
    "    ,'Haas F1 Team': '#AEAEAE'\n",
    "    ,'Williams': '#00A3E0'\n",
    "}\n",
    "\n",
    "# Colormap\n",
    "CMAP = (\n",
    "    mcolors\n",
    "    .LinearSegmentedColormap\n",
    "    .from_list(\n",
    "        \"my_cmap\"\n",
    "        ,[BLUE, YELLOW, RED]\n",
    "    )\n",
    ")\n",
    "\n",
    "COMPOUND_COLORS = {\n",
    "    'SOFT': '#F20704'\n",
    "    ,'MEDIUM': '#FACA08'\n",
    "    ,'HARD': '#000000'\n",
    "    ,'INTERMEDIATE': '#029405'\n",
    "    ,'WET': '#078CD1'\n",
    "}\n",
    "\n",
    "# Plot constants\n",
    "EVENT_PLT = 'British Grand Prix'\n",
    "YEAR_PLT = 2022\n",
    "DRIVER_PLT = 'VER'\n",
    "\n",
    "# Features\n",
    "LAP_METRIC = [\n",
    "    'LapTime'\n",
    "    ,'LapNumber'\n",
    "    ,'Stint'\n",
    "    ,'PitOutTime'\n",
    "    ,'PitInTime'\n",
    "    ,'Sector1Time'\n",
    "    ,'Sector2Time'\n",
    "    ,'Sector3Time'\n",
    "    ,'SpeedI1'\n",
    "    ,'SpeedI2'\n",
    "    ,'SpeedFL'\n",
    "    ,'SpeedST'\n",
    "    ,'TyreLife'\n",
    "    ,'LapStartTime'\n",
    "    ,'LapSessionTime'\n",
    "]\n",
    "LAP_CATEGORICAL = [\n",
    "    'Driver'\n",
    "    ,'DriverNumber'\n",
    "    ,'IsPersonalBest'\n",
    "    ,'Compound'\n",
    "    ,'FreshTyre'\n",
    "    ,'Team'\n",
    "    ,'TrackStatus'\n",
    "    ,'Position'\n",
    "    ,'Deleted'\n",
    "    ,'FastF1Generated'\n",
    "    ,'IsAccurate'\n",
    "    ,'Year'\n",
    "    ,'EventName'\n",
    "    ,'Session'\n",
    "]\n",
    "TELEMETRY_METRIC = [\n",
    "    'RPM'\n",
    "    ,'Speed'\n",
    "    ,'Throttle'\n",
    "    ,'SessionTime'\n",
    "    ,'Distance'\n",
    "    ,'LapNumber'\n",
    "]\n",
    "TELEMETRY_CATEGORICAL = [\n",
    "    'nGear'\n",
    "    ,'Brake'\n",
    "    ,'DRS'\n",
    "    ,'Driver'\n",
    "    ,'Year'\n",
    "    ,'EventName'\n",
    "    ,'Session'\n",
    "    ,'IsDRSActive'\n",
    "]\n",
    "WEATHER_METRIC = [\n",
    "    'Time'\n",
    "    ,'AirTemp'\n",
    "    ,'Humidity'\n",
    "    ,'Pressure'\n",
    "    ,'TrackTemp'\n",
    "    ,'WindSpeed'\n",
    "]\n",
    "WEATHER_CATEGORICAL = [\n",
    "    'Rainfall'\n",
    "    ,'WindDirection'\n",
    "    ,'Year'\n",
    "    ,'EventName'\n",
    "    ,'Session'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fce78",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9fc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for number of bins in histogram\n",
    "def sturges_bins(data, column_name):\n",
    "    \"\"\"\n",
    "    Calculates the number of bins for a histogram using Sturges' Rule.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataset containing the column.\n",
    "    column_name (str): The name of the column to calculate the number of bins for.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of bins calculated using Sturges' Rule.\n",
    "    \"\"\"\n",
    "    # Number of data points\n",
    "    n = len(data[column_name])\n",
    "    \n",
    "    # Calculate the number of bins using Sturges' Rule\n",
    "    k = int(np.ceil(np.log2(n) + 1))\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ad450",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "In this section, the spark session is initiliazed and the data is ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db9391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lap Data Aggregation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ea3b7",
   "metadata": {},
   "source": [
    "`TODO`: Implement data to be loaded from databricks\n",
    "\n",
    "Upload the file through the UI\n",
    "- Go to the Data tab on the left sidebar in your Databricks workspace.\n",
    "- Click \"Add Data\" â†’ then choose \"Upload File\".\n",
    "- Upload your file (e.g., CSV, JSON, Parquet).\n",
    "- Databricks will store it in something like:\n",
    "    /FileStore/tables/your_filename.csv\n",
    "\n",
    "You can then access it like this:\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/FileStore/tables/your_filename.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30b365",
   "metadata": {},
   "source": [
    "### 1.1. Lap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dd3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lap_data = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATA_DIR}/laps.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f0595",
   "metadata": {},
   "source": [
    "#### 1.1.1. Feature Analysis\n",
    "\n",
    "After analysing the available metadata, some features are removed from the dataframe, as they do not bring necessary value to the problem solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5057e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "lap_data = lap_data.drop(\n",
    "    \"Sector1SessionTime\"\n",
    "    ,\"Sector2SessionTime\"\n",
    "    ,\"Sector3SessionTime\"\n",
    "    ,\"LapStartDate\"\n",
    "    ,\"DeletedReason\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7abdd",
   "metadata": {},
   "source": [
    "#### 1.1.2. Fixing Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8454b80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Driver: string (nullable = true)\n",
      " |-- DriverNumber: integer (nullable = true)\n",
      " |-- LapTime: string (nullable = true)\n",
      " |-- LapNumber: double (nullable = true)\n",
      " |-- Stint: double (nullable = true)\n",
      " |-- PitOutTime: string (nullable = true)\n",
      " |-- PitInTime: string (nullable = true)\n",
      " |-- Sector1Time: string (nullable = true)\n",
      " |-- Sector2Time: string (nullable = true)\n",
      " |-- Sector3Time: string (nullable = true)\n",
      " |-- SpeedI1: double (nullable = true)\n",
      " |-- SpeedI2: double (nullable = true)\n",
      " |-- SpeedFL: double (nullable = true)\n",
      " |-- SpeedST: double (nullable = true)\n",
      " |-- IsPersonalBest: boolean (nullable = true)\n",
      " |-- Compound: string (nullable = true)\n",
      " |-- TyreLife: double (nullable = true)\n",
      " |-- FreshTyre: boolean (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- LapStartTime: string (nullable = true)\n",
      " |-- TrackStatus: integer (nullable = true)\n",
      " |-- Position: double (nullable = true)\n",
      " |-- Deleted: boolean (nullable = true)\n",
      " |-- FastF1Generated: boolean (nullable = true)\n",
      " |-- IsAccurate: boolean (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- EventName: string (nullable = true)\n",
      " |-- Session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check datatypes\n",
    "lap_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7142a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datatypes\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"LapSessionTime\", split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapTime\", split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"LapTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapNumber\", col(\"LapNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"Stint\", col(\"Stint\").cast(IntegerType()))\n",
    "    .withColumn(\"PitOutTime\", split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"PitOutTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"PitInTime\", split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"PitInTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector1Time\", split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector1Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector2Time\", split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector2Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Sector3Time\", split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Sector3Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"SpeedI1\", col(\"SpeedI1\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedI2\", col(\"SpeedI2\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedFL\", col(\"SpeedFL\").cast(IntegerType()))\n",
    "    .withColumn(\"SpeedST\", col(\"SpeedST\").cast(IntegerType()))\n",
    "    .withColumn(\"TyreLife\", col(\"TyreLife\").cast(IntegerType()))\n",
    "    .withColumn(\"LapStartTime\", split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"LapStartTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Position\", col(\"Position\").cast(IntegerType()))\n",
    "    .withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
    ")\n",
    "\n",
    "lap_data = lap_data.drop(col(\"Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1fac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+---------------+------------+-----------+--------+-------+---------------+----------+----+------------------+-------+--------------+\n",
      "|Driver|DriverNumber|           LapTime|LapNumber|Stint|PitOutTime|PitInTime|Sector1Time|Sector2Time|Sector3Time|SpeedI1|SpeedI2|SpeedFL|SpeedST|IsPersonalBest|Compound|TyreLife|FreshTyre|           Team|LapStartTime|TrackStatus|Position|Deleted|FastF1Generated|IsAccurate|Year|         EventName|Session|LapSessionTime|\n",
      "+------+------------+------------------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+---------------+------------+-----------+--------+-------+---------------+----------+----+------------------+-------+--------------+\n",
      "|   VER|           1|100.23599999999999|        1|    1|      NULL|     NULL|       NULL|     42.325|     24.389|    230|    254|    274|    250|         false|    SOFT|       4|    false|Red Bull Racing|    3754.872|          1|       2|  false|          false|     false|2022|Bahrain Grand Prix|      R|       3855.34|\n",
      "+------+------------+------------------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+---------------+------------+-----------+--------+-------+---------------+----------+----+------------------+-------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "lap_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c399a0",
   "metadata": {},
   "source": [
    "### 1.2. Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511825c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_data = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATA_DIR}/telemetry.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d6a1f",
   "metadata": {},
   "source": [
    "#### 1.2.1. Feature Analysis\n",
    "\n",
    "After analysing the available metadata, some features are removed from the dataframe, as they do not bring necessary value to the problem solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac9f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_data = telemetry_data.drop(\n",
    "    \"Date\"\n",
    "    ,\"DataCollectionTime\"\n",
    "    ,\"Time\"\n",
    "    ,\"Source\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea37f22",
   "metadata": {},
   "source": [
    "#### 1.2.2. Fixing Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d1d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RPM: double (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- nGear: integer (nullable = true)\n",
      " |-- Throttle: double (nullable = true)\n",
      " |-- Brake: boolean (nullable = true)\n",
      " |-- DRS: integer (nullable = true)\n",
      " |-- SessionTime: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Driver: string (nullable = true)\n",
      " |-- LapNumber: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- EventName: string (nullable = true)\n",
      " |-- Session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check datatypes\n",
    "telemetry_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acac3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"RPM\", col(\"RPM\").cast(IntegerType()))\n",
    "    .withColumn(\"Speed\", col(\"Speed\").cast(IntegerType()))\n",
    "    .withColumn(\"Throttle\", col(\"Throttle\").cast(IntegerType()))\n",
    "    .withColumn(\"Brake\", col(\"Brake\").cast(BooleanType()).cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"SessionTime\", split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"SessionTime\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"LapNumber\", col(\"LapNumber\").cast(IntegerType()))\n",
    "    .withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"IsDRSActive\", when(\n",
    "            col(\"DRS\").isin(10, 12, 14), 1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301ffa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------+-----+---+-----------+--------+------+---------+----+------------------+-------+-----------+\n",
      "| RPM|Speed|nGear|Throttle|Brake|DRS|SessionTime|Distance|Driver|LapNumber|Year|         EventName|Session|IsDRSActive|\n",
      "+----+-----+-----+--------+-----+---+-----------+--------+------+---------+----+------------------+-------+-----------+\n",
      "|9802|    0|    1|      10|    1|  1|   3754.971|     0.0|   VER|        1|2022|Bahrain Grand Prix|      R|          0|\n",
      "+----+-----+-----+--------+-----+---+-----------+--------+------+---------+----+------------------+-------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "telemetry_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe71e9",
   "metadata": {},
   "source": [
    "### 1.3. Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ab7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATA_DIR}/weather.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271cc56b",
   "metadata": {},
   "source": [
    "#### 1.3.1. Fixing Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1daf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: string (nullable = true)\n",
      " |-- AirTemp: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Pressure: double (nullable = true)\n",
      " |-- Rainfall: boolean (nullable = true)\n",
      " |-- TrackTemp: double (nullable = true)\n",
      " |-- WindDirection: integer (nullable = true)\n",
      " |-- WindSpeed: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- EventName: string (nullable = true)\n",
      " |-- Session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check datatypes\n",
    "weather_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a5bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = (\n",
    "    weather_data\n",
    "    .withColumn(\n",
    "        \"Time\", split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(0).cast(\"int\") * 3600 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(1).cast(\"int\") * 60 +\n",
    "        split(regexp_replace(col(\"Time\"), r\"^0 days \", \"\"), \":\").getItem(2).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\"Rainfall\", col(\"Rainfall\").cast(IntegerType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50743fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+--------+--------+---------+-------------+---------+----+------------------+-------+\n",
      "|  Time|AirTemp|Humidity|Pressure|Rainfall|TrackTemp|WindDirection|WindSpeed|Year|         EventName|Session|\n",
      "+------+-------+--------+--------+--------+---------+-------------+---------+----+------------------+-------+\n",
      "|63.204|   25.6|    17.0|  1010.2|       0|     32.3|          346|      0.5|2022|Bahrain Grand Prix|      R|\n",
      "+------+-------+--------+--------+--------+---------+-------------+---------+----+------------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "weather_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3254c",
   "metadata": {},
   "source": [
    "## 2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f32ea",
   "metadata": {},
   "source": [
    "### 2.1. Lap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f98f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to pandas for visualization purposes\n",
    "lap_pd = lap_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a47ca8",
   "metadata": {},
   "source": [
    "#### 2.1.1. Histograms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "962e9469",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Subplot layout\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(LAP_METRIC) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram and density line\n",
    "for i, col_ in enumerate(LAP_METRIC):\n",
    "    sns.histplot(\n",
    "        data=lap_pd\n",
    "        ,x=col_\n",
    "        ,ax=axes[i]\n",
    "        ,bins=sturges_bins(lap_pd, col_)\n",
    "        ,color=BLUE\n",
    "        ,edgecolor=YELLOW\n",
    "        ,stat='density'\n",
    "        ,alpha=1\n",
    "    )\n",
    "\n",
    "    sns.kdeplot(\n",
    "        data=lap_pd\n",
    "        ,x=col_\n",
    "        ,ax=axes[i]\n",
    "        ,color=RED\n",
    "        ,linewidth=2\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_xlabel(col_)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distributions of Numeric Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b13c163c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Plot for categorical features\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(LAP_CATEGORICAL) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col_ in enumerate(LAP_CATEGORICAL):\n",
    "    sns.countplot(\n",
    "        data=lap_pd[lap_pd[col_].notna()],\n",
    "        x=col_,\n",
    "        ax=axes[i],\n",
    "        color=BLUE\n",
    "    )\n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_xlabel(col_)\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "    axes[i].tick_params(axis='x', rotation=90)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distributions of Categorical Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09e5cb",
   "metadata": {},
   "source": [
    "#### 2.1.2. Boxplots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "640fd1c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Subplot layout\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(LAP_METRIC) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot boxplots\n",
    "for i, col_ in enumerate(LAP_METRIC):\n",
    "    sns.boxplot(y=lap_pd[col_], ax=axes[i], color=BLUE, boxprops=dict(edgecolor=BLUE),\n",
    "        whiskerprops=dict(color=BLUE),\n",
    "        capprops=dict(color=RED),\n",
    "        medianprops=dict(color=YELLOW),\n",
    "        flierprops=dict(markerfacecolor=BLUE, markeredgecolor=BLUE))\n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_ylabel(col_)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Boxplots of Numeric Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56482c61",
   "metadata": {},
   "source": [
    "#### 2.1.3. Other Plots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84504299",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Filter data\n",
    "race_df = lap_pd[\n",
    "    (lap_pd['EventName'] == EVENT_PLT) & \n",
    "    (lap_pd['Year'] == YEAR_PLT)\n",
    "].copy()\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(race_df['Driver'].unique()) / n_cols)\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows), sharex=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, driver in enumerate(race_df['Driver'].unique()):\n",
    "    ax = axes[i]\n",
    "    driver_laps = race_df[race_df['Driver'] == driver].sort_values('LapNumber')\n",
    "\n",
    "    # Plot lap time\n",
    "    sns.lineplot(\n",
    "        data=driver_laps\n",
    "        ,x='LapNumber'\n",
    "        ,y='LapTime'\n",
    "        ,color=BLUE\n",
    "        ,linewidth=2\n",
    "        ,ax=ax\n",
    "    )\n",
    "\n",
    "    # Add vertical lines for pit stop\n",
    "    pit_laps = driver_laps[driver_laps['PitOutTime'].notnull()]['LapNumber'].unique()\n",
    "    for lap in pit_laps:\n",
    "        ax.axvline(x=lap, color=RED, linestyle='-', linewidth=2)\n",
    "\n",
    "    # Shade laps with track status different than Green Flag\n",
    "    non_green_laps = driver_laps[driver_laps['TrackStatus'] != 1]['LapNumber'].values\n",
    "    if len(non_green_laps) > 0:\n",
    "        start = non_green_laps[0]\n",
    "        for j in range(1, len(non_green_laps)):\n",
    "            if non_green_laps[j] != non_green_laps[j-1] + 1:\n",
    "                ax.axvspan(start, non_green_laps[j-1] + 1, color=YELLOW, alpha=0.3)\n",
    "                start = non_green_laps[j]\n",
    "        ax.axvspan(start, non_green_laps[-1] + 1, color=YELLOW, alpha=0.3)\n",
    "\n",
    "    ax.set_title(f\"{driver}\")\n",
    "    ax.set_xlabel(\"Lap Number\")\n",
    "    ax.set_ylabel(\"Lap Time\")\n",
    "    ax.set_xticks(range(driver_laps['LapNumber'].min(), driver_laps['LapNumber'].max() + 1, 2))\n",
    "\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Add suptitle\n",
    "fig.suptitle(f\"Lap Times per Driver â€“ {EVENT_PLT} {YEAR_PLT}\", fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0fff6",
   "metadata": {},
   "source": [
    "This plot shows the majority of drivers pit when there is a track status change. All drivers racing during the 3rd moment of track status change but two decided to pit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ababb3b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "race_df['AdjustedLapTime'] = np.where(\n",
    "    race_df['PitOutTime'].notna()\n",
    "    ,race_df['LapSessionTime'] - race_df['LapStartTime'] - (race_df['PitOutTime'] - race_df['LapStartTime'])\n",
    "    ,race_df['LapSessionTime'] - race_df['LapStartTime']\n",
    ")\n",
    "\n",
    "# Pivot the data\n",
    "heatmap_data = race_df.pivot_table(\n",
    "    index='Driver',\n",
    "    columns='LapNumber',\n",
    "    values='AdjustedLapTime'\n",
    ")\n",
    "\n",
    "# Create dataframe for annotations\n",
    "annotations = pd.DataFrame(\n",
    "    ''\n",
    "    ,index=heatmap_data.index\n",
    "    ,columns=heatmap_data.columns\n",
    ")\n",
    "# Filter laps with pit stop\n",
    "pitstops = race_df[\n",
    "    (race_df['PitOutTime'].notnull())\n",
    "]\n",
    "\n",
    "# Set 'PS' on pit stop laps\n",
    "for _, row in pitstops.iterrows():\n",
    "    driver = row['Driver']\n",
    "    lap = row['LapNumber']\n",
    "    if driver in annotations.index and lap in annotations.columns:\n",
    "        annotations.at[driver, lap] = 'PS'\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data\n",
    "    ,cmap=CMAP\n",
    "    ,linewidths=0.2\n",
    "    ,linecolor='gray'\n",
    "    ,cbar_kws={'label': 'Lap Time (s)'}\n",
    "    ,annot=annotations\n",
    "    ,fmt=''\n",
    "    ,annot_kws={\n",
    "        \"size\": 8\n",
    "        ,\"weight\": \"bold\"\n",
    "        ,\"color\": \"black\"\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.text(\n",
    "    x=heatmap_data.shape[1] + 1.5\n",
    "    ,y=heatmap_data.shape[0] + 0.8\n",
    "    ,s=\"PS = Pit Stop\"\n",
    "    ,fontsize=10\n",
    "    ,fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.title(f'Lap Times Heatmap â€“ {EVENT_PLT} {YEAR_PLT}', fontsize=16)\n",
    "plt.xlabel('Lap Number')\n",
    "plt.ylabel('Driver')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef672138",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Pivot the data\n",
    "heatmap_data = race_df.pivot_table(\n",
    "    index='Driver',\n",
    "    columns='LapNumber',\n",
    "    values='TyreLife'\n",
    ")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=CMAP,\n",
    "    linewidths=0.2,\n",
    "    linecolor='gray',\n",
    "    cbar_kws={'label': 'Tyre Life (laps)'}\n",
    ")\n",
    "\n",
    "plt.title(f'Tyre Life Heatmap â€“ {EVENT_PLT} {YEAR_PLT}')\n",
    "plt.xlabel('Lap Number')\n",
    "plt.ylabel('Driver')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e3997bd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Filter the data\n",
    "tyre_life = race_df[(race_df['LapTime'].notna())].sort_values(by='LapNumber')\n",
    "\n",
    "# Determine the number of rows needed\n",
    "n_cols = 3\n",
    "n_rows = (len(tyre_life['Driver'].unique()) + n_cols - 1) // n_cols\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each driver and plot\n",
    "for i, driver in enumerate(tyre_life['Driver'].unique()):\n",
    "    # Filter data for the current driver\n",
    "    driver_data = tyre_life[tyre_life['Driver'] == driver]\n",
    "\n",
    "    # Create scatter plot for the driver with hue\n",
    "    sns.scatterplot(\n",
    "        x='TyreLife', y='LapTime',\n",
    "        data=driver_data,\n",
    "        hue='Compound',\n",
    "        palette=COMPOUND_COLORS,\n",
    "        ax=axes[i]\n",
    "    )\n",
    "\n",
    "    # Add a regression line (single trend, not per compound)\n",
    "    sns.regplot(\n",
    "        x='TyreLife', y='LapTime',\n",
    "        data=driver_data,\n",
    "        scatter=False,\n",
    "        color=BLUE,\n",
    "        ax=axes[i],\n",
    "        line_kws={'label': 'Trend Line'}\n",
    "    )\n",
    "\n",
    "    # Set title and labels for each subplot\n",
    "    axes[i].set_title(f'{driver}', fontsize=14)\n",
    "    axes[i].set_xlabel('Tyre Life (Laps)', fontsize=12)\n",
    "    axes[i].set_ylabel('Lap Time (Seconds)', fontsize=12)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "    # Combine legends from both plots (scatter + regplot)\n",
    "    handles, labels = axes[i].get_legend_handles_labels()\n",
    "    # Remove duplicates and preserve order\n",
    "    seen = set()\n",
    "    new_handles, new_labels = [], []\n",
    "    for h, l in zip(handles, labels):\n",
    "        if l not in seen:\n",
    "            new_handles.append(h)\n",
    "            new_labels.append(l)\n",
    "            seen.add(l)\n",
    "    axes[i].legend(new_handles, new_labels, title='Compound')\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Tyre Life vs. Lap Time - {EVENT_PLT} {YEAR_PLT}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832ccf7",
   "metadata": {},
   "source": [
    "By analysing this plot, it is possible to conclude the first couple of laps on a new set of tyres are usually very slow compared to the remaining."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b25f210b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Create a dictionary that maps each driver to their team color\n",
    "driver_team_colors = race_df.dropna(subset=['Team']).drop_duplicates(subset=['Driver'])\n",
    "driver_team_colors['Color'] = driver_team_colors['Team'].map(TEAM_COLORS)\n",
    "\n",
    "# Create a dictionary for driver names and their corresponding team color\n",
    "driver_color_map = driver_team_colors.set_index('Driver')['Color'].to_dict()\n",
    "\n",
    "# Plot the data with team colors\n",
    "plt.figure(figsize=(20, 9))\n",
    "\n",
    "# Lineplot of Position vs LapNumber, colored by Driver's team color\n",
    "sns.lineplot(\n",
    "    data=race_df\n",
    "    ,x='LapNumber'\n",
    "    ,y='Position'\n",
    "    ,hue='Driver'\n",
    "    ,palette=driver_color_map  # Use the dictionary with Driver:Color mapping\n",
    "    ,marker='o'\n",
    "    ,linewidth=2\n",
    "    ,alpha=0.9\n",
    "    ,legend=False\n",
    ")\n",
    "\n",
    "# Loop over each driver for labels\n",
    "for driver in race_df['Driver'].unique():\n",
    "    driver_data = race_df[race_df['Driver'] == driver]\n",
    "\n",
    "    # End of the line\n",
    "    last_lap = driver_data['LapNumber'].max()\n",
    "    last_pos = driver_data.loc[driver_data['LapNumber'] == last_lap, 'Position'].values[0]\n",
    "    if last_lap == race_df['LapNumber'].max():\n",
    "        plt.text(\n",
    "            x=last_lap + 0.5\n",
    "            ,y=last_pos\n",
    "            ,s=driver\n",
    "            ,fontsize=12\n",
    "            ,va='center'\n",
    "        )\n",
    "\n",
    "    # Start of the line\n",
    "    first_lap = driver_data['LapNumber'].min()\n",
    "    first_pos = driver_data.loc[driver_data['LapNumber'] == first_lap, 'Position'].values[0]\n",
    "    plt.text(\n",
    "        x=first_lap - 1.5\n",
    "        ,y=first_pos\n",
    "        ,s=driver\n",
    "        ,fontsize=12\n",
    "        ,va='center'\n",
    "    )\n",
    "\n",
    "# Invert y-axis: Position 1 should be at the top\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Set y-axis ticks to be each integer value (positions 1 to max)\n",
    "plt.yticks(range(1, int(race_df['Position'].max()) + 1))\n",
    "\n",
    "# Set x-axis to start at 1 and mark each unit\n",
    "plt.xticks(range(1, int(race_df['LapNumber'].max()) + 1))\n",
    "\n",
    "# Add vertical dashed lines every 5 units on the x-axis\n",
    "for lap in range(1, int(race_df['LapNumber'].max()) + 1):\n",
    "    if lap == 1 or lap % 5 == 0 or lap == int(race_df['LapNumber'].max()):\n",
    "        plt.axvline(x=lap, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(f'Driver Position Over Laps â€“ {EVENT_PLT} {YEAR_PLT}', fontsize=16, y=1.02)\n",
    "plt.xlabel('Lap Number')\n",
    "plt.ylabel('Track Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c28dda44",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Calculate correlation matrix between LapTime and speed-related features\n",
    "speed_corr = race_df[['LapTime', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']].corr()\n",
    "\n",
    "# Create mask for upper triangle\n",
    "mask = np.triu(np.ones_like(speed_corr, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    speed_corr\n",
    "    ,annot=True\n",
    "    ,cmap=CMAP\n",
    "    ,vmin=-1\n",
    "    ,vmax=1\n",
    "    ,fmt=\".2f\"\n",
    "    ,linewidths=0.5\n",
    "    ,cbar_kws={\"label\": \"Correlation Coefficient\"}\n",
    "    ,mask=mask\n",
    ")\n",
    "plt.title(f\"Correlation between Lap Time and Speed Metrics - {EVENT_PLT} {YEAR_PLT}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dde6f1",
   "metadata": {},
   "source": [
    "### 2.2. Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdebd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data, since the dataset is too large and convert to pandas for visualization purposes\n",
    "telemetry_pd = (\n",
    "    telemetry_data\n",
    "    .filter(\n",
    "        (col(\"EventName\") == EVENT_PLT) &\n",
    "        (col(\"Year\") == YEAR_PLT)\n",
    "    )\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716058ea",
   "metadata": {},
   "source": [
    "#### 2.2.1. Histograms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26307e71",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Subplot layout\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(TELEMETRY_METRIC) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram and density line\n",
    "for i, col_ in enumerate(TELEMETRY_METRIC):\n",
    "    sns.histplot(\n",
    "        data=telemetry_pd\n",
    "        ,x=col_\n",
    "        ,ax=axes[i]\n",
    "        ,bins=sturges_bins(telemetry_pd, col_)\n",
    "        ,color=BLUE\n",
    "        ,edgecolor=YELLOW\n",
    "        ,stat='density'\n",
    "        ,alpha=1\n",
    "    )\n",
    "\n",
    "    sns.kdeplot(\n",
    "        data=telemetry_pd\n",
    "        ,x=col_\n",
    "        ,ax=axes[i]\n",
    "        ,color=RED\n",
    "        ,linewidth=2\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_xlabel(col_)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distributions of Numeric Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e95db87",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Plot for categorical features\n",
    "n_cols_cat = 3\n",
    "n_rows_cat = int(np.ceil(len(TELEMETRY_CATEGORICAL) / n_cols_cat))\n",
    "fig_cat, axes_cat = plt.subplots(n_rows_cat, n_cols_cat, figsize=(15, n_rows_cat * 4))\n",
    "axes_cat = axes_cat.flatten()\n",
    "\n",
    "for i, col_ in enumerate(TELEMETRY_CATEGORICAL):\n",
    "    sns.countplot(\n",
    "        data=telemetry_pd[telemetry_pd[col_].notna()],\n",
    "        x=col_,\n",
    "        ax=axes_cat[i],\n",
    "        color=BLUE\n",
    "    )\n",
    "    axes_cat[i].set_title(col_)\n",
    "    axes_cat[i].set_xlabel(col_)\n",
    "    axes_cat[i].set_ylabel(\"Count\")\n",
    "    axes_cat[i].tick_params(axis='x', rotation=90)\n",
    "\n",
    "for j in range(i + 1, len(axes_cat)):\n",
    "    axes_cat[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distributions of Categorical Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7dcfa5",
   "metadata": {},
   "source": [
    "#### 2.2.2. Boxplots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cbc20d3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Subplot layout\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(TELEMETRY_METRIC) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot boxplots\n",
    "for i, col_ in enumerate(TELEMETRY_METRIC):\n",
    "    sns.boxplot(y=telemetry_pd[col_], ax=axes[i], color=BLUE, boxprops=dict(edgecolor=BLUE),\n",
    "        whiskerprops=dict(color=BLUE),\n",
    "        capprops=dict(color=RED),\n",
    "        medianprops=dict(color=YELLOW),\n",
    "        flierprops=dict(markerfacecolor=BLUE, markeredgecolor=BLUE))\n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_ylabel(col_)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Boxplots of Numeric Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc67ec",
   "metadata": {},
   "source": [
    "#### 2.2.3. Other Plots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9159f666",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_plot = telemetry_pd[(telemetry_pd['Driver'] == DRIVER_PLT)]\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Plot Speed\n",
    "sns.lineplot(data=df_plot, x=\"LapNumber\", y=\"Speed\", ax=axs[0], color=BLUE)\n",
    "axs[0].set_title(f\"Speed over Lap Number\")\n",
    "axs[0].set_ylabel(\"Speed (km/h)\")\n",
    "\n",
    "# Plot Throttle\n",
    "sns.lineplot(data=df_plot, x=\"LapNumber\", y=\"Throttle\", ax=axs[1], color=YELLOW)\n",
    "axs[1].set_title(\"Throttle over Lap Number\")\n",
    "axs[1].set_ylabel(\"Throttle (%)\")\n",
    "\n",
    "# Plot RPM\n",
    "sns.lineplot(data=df_plot, x=\"LapNumber\", y=\"RPM\", ax=axs[2], color=RED)\n",
    "axs[2].set_title(\"RPM over Lap Number\")\n",
    "axs[2].set_ylabel(\"RPM\")\n",
    "axs[2].set_xlabel(\"Lap Number\")\n",
    "\n",
    "# Plot Brake\n",
    "sns.lineplot(data=df_plot, x=\"LapNumber\", y=\"Brake\", ax=axs[3], color=BLUE)\n",
    "axs[3].set_title(\"Brake over Lap Number\")\n",
    "axs[3].set_ylabel(\"Brake\")\n",
    "axs[3].set_xlabel(\"Lap Number\")\n",
    "\n",
    "plt.suptitle(f\"Speed, Throttle, RPM and Brake - {EVENT_PLT} {YEAR_PLT}, {DRIVER_PLT}\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57585848",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Filter data\n",
    "drs_plot = df_plot[(df_plot['LapNumber'] >= 7) & (df_plot['LapNumber'] <= 12)]\n",
    "\n",
    "# First, identify where LapNumber changes (lap start points)\n",
    "lap_changes = drs_plot[['SessionTime', 'LapNumber']].drop_duplicates('LapNumber')\n",
    "\n",
    "# Prepare segments for LineCollection\n",
    "x = drs_plot['SessionTime'].values\n",
    "y = drs_plot['Speed'].values\n",
    "c = drs_plot['IsDRSActive'].astype(int).values\n",
    "\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "# Split into DRS On and Off segments\n",
    "segments_on = [seg for i, seg in enumerate(segments) if c[i] == 1]\n",
    "segments_off = [seg for i, seg in enumerate(segments) if c[i] == 0]\n",
    "\n",
    "# Create separate LineCollections\n",
    "lc_on = LineCollection(segments_on, colors=RED, linewidth=2, label='DRS On')\n",
    "lc_off = LineCollection(segments_off, colors=BLUE, linewidth=2, label='DRS Off')\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.add_collection(lc_on)\n",
    "ax.add_collection(lc_off)\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.set_ylim(y.min(), y.max())\n",
    "ax.set_title(f\"DRS Speed Influence - {EVENT_PLT} {YEAR_PLT}, {DRIVER_PLT}\", fontsize=16)\n",
    "ax.set_xlabel(\"Session Time (s)\")\n",
    "ax.set_ylabel(\"Speed (km/h)\")\n",
    "\n",
    "# Add vertical lines for each lap change\n",
    "for _, row in lap_changes.iterrows():\n",
    "    ax.axvline(x=row['SessionTime'] - 2, color='grey', linestyle='--', alpha=0.7)\n",
    "    ax.text(row['SessionTime'], y.max(), f\"Lap {int(row['LapNumber'])}\", \n",
    "            rotation=90, verticalalignment='top', horizontalalignment='left', fontsize=9, color='grey')\n",
    "\n",
    "# Add manual legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=RED, lw=2, label='DRS On'),\n",
    "    Line2D([0], [0], color=BLUE, lw=2, label='DRS Off')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "765dc672",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Select one example lap\n",
    "gear_df = df_plot[df_plot['LapNumber'] == 25].sort_values('Distance')\n",
    "\n",
    "# Identify gear shifts\n",
    "gear_diff = gear_df['nGear'].diff().fillna(0)\n",
    "gear_shift_indices = gear_diff != 0\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot the gear line\n",
    "plt.plot(gear_df['Distance'], gear_df['nGear'], linestyle='-', color=BLUE)\n",
    "\n",
    "# Overlay markers only at gear shift points\n",
    "plt.plot(\n",
    "    gear_df['Distance'][gear_shift_indices]\n",
    "    ,gear_df['nGear'][gear_shift_indices]\n",
    "    ,marker='o'\n",
    "    ,linestyle='None'\n",
    "    ,color=RED\n",
    "    ,label='Gear Shift'\n",
    ")\n",
    "\n",
    "# Styling\n",
    "plt.title(f'Gear Change Analysis {EVENT_PLT} {YEAR_PLT} - {DRIVER_PLT} Lap 25', fontsize=16)\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Gear')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d032045",
   "metadata": {},
   "source": [
    "### 2.3. Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd6a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data, since the dataset is too large and convert to pandas for visualization purposes\n",
    "weather_pd = (\n",
    "    weather_data\n",
    "    .filter(\n",
    "        (col(\"EventName\") == EVENT_PLT) &\n",
    "        (col(\"Year\") == YEAR_PLT)\n",
    "    )\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a6e4f",
   "metadata": {},
   "source": [
    "#### 2.3.1. Histograms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddc36beb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Subplot layout\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(WEATHER_METRIC) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram and density line\n",
    "for i, col_ in enumerate(WEATHER_METRIC):\n",
    "    sns.histplot(\n",
    "        data=weather_pd\n",
    "        ,x=col_\n",
    "        ,ax=axes[i]\n",
    "        ,bins=sturges_bins(weather_pd, col_)\n",
    "        ,color=BLUE\n",
    "        ,edgecolor=YELLOW\n",
    "        ,stat='density'\n",
    "        ,alpha=1\n",
    "    )\n",
    "\n",
    "    sns.kdeplot(\n",
    "        data=weather_pd\n",
    "        ,x=col_\n",
    "        ,ax=axes[i]\n",
    "        ,color=RED\n",
    "        ,linewidth=2\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_xlabel(col_)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distributions of Numeric Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3de0834c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Plot for categorical features\n",
    "n_cols_cat = 3\n",
    "n_rows_cat = int(np.ceil(len(WEATHER_CATEGORICAL) / n_cols_cat))\n",
    "fig_cat, axes_cat = plt.subplots(n_rows_cat, n_cols_cat, figsize=(15, n_rows_cat * 4))\n",
    "axes_cat = axes_cat.flatten()\n",
    "\n",
    "for i, col_ in enumerate(WEATHER_CATEGORICAL):\n",
    "    sns.countplot(\n",
    "        data=weather_pd[weather_pd[col_].notna()],\n",
    "        x=col_,\n",
    "        ax=axes_cat[i],\n",
    "        color=BLUE\n",
    "    )\n",
    "    axes_cat[i].set_title(col_)\n",
    "    axes_cat[i].set_xlabel(col_)\n",
    "    axes_cat[i].set_ylabel(\"Count\")\n",
    "    axes_cat[i].tick_params(axis='x', rotation=90)\n",
    "\n",
    "for j in range(i + 1, len(axes_cat)):\n",
    "    axes_cat[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distributions of Categorical Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e2c80",
   "metadata": {},
   "source": [
    "#### 2.2.2. Boxplots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61b012e1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Subplot layout\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(WEATHER_METRIC) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot boxplots\n",
    "for i, col_ in enumerate(WEATHER_METRIC):\n",
    "    sns.boxplot(y=weather_pd[col_], ax=axes[i], color=BLUE, boxprops=dict(edgecolor=BLUE),\n",
    "        whiskerprops=dict(color=BLUE),\n",
    "        capprops=dict(color=RED),\n",
    "        medianprops=dict(color=YELLOW),\n",
    "        flierprops=dict(markerfacecolor=BLUE, markeredgecolor=BLUE))\n",
    "    axes[i].set_title(col_)\n",
    "    axes[i].set_ylabel(col_)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Boxplots of Numeric Features\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd69259",
   "metadata": {},
   "source": [
    "#### 2.2.3. Other Plots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef808c28",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Set up plot grid\n",
    "n_metrics = len(WEATHER_METRIC)\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(n_metrics / n_cols)\n",
    "\n",
    "plt.figure(figsize=(14, 4 * n_rows))\n",
    "\n",
    "for i, metric in enumerate(WEATHER_METRIC, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.lineplot(data=weather_pd, x='Time', y=metric, color=BLUE)\n",
    "    plt.title(f'{metric} over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(metric)\n",
    "\n",
    "plt.suptitle(f\"Weather Data - {EVENT_PLT} {YEAR_PLT}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5304b23b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# AirTemp vs TrackTemp\n",
    "sns.scatterplot(\n",
    "    data=weather_pd\n",
    "    ,x='AirTemp'\n",
    "    ,y='TrackTemp'\n",
    "    ,hue='Rainfall'\n",
    "    ,size='WindSpeed'\n",
    "    ,palette={\n",
    "        True: BLUE\n",
    "        ,False: RED\n",
    "    }\n",
    "    ,sizes=(20, 200)\n",
    "    ,ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('AirTemp vs TrackTemp')\n",
    "axes[0].set_xlabel('Air Temperature')\n",
    "axes[0].set_ylabel('Track Temperature')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Humidity vs Pressure\n",
    "sns.scatterplot(\n",
    "    data=weather_pd\n",
    "    ,x='Humidity'\n",
    "    ,y='Pressure'\n",
    "    ,hue='Rainfall'\n",
    "    ,size='WindSpeed'\n",
    "    ,palette={\n",
    "        True: BLUE\n",
    "        ,False: RED\n",
    "    }\n",
    "    ,sizes=(20, 200)\n",
    "    ,ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Humidity vs Pressure')\n",
    "axes[1].set_xlabel('Humidity')\n",
    "axes[1].set_ylabel('Pressure')\n",
    "\n",
    "plt.suptitle(f\"Weather Conditions - {EVENT_PLT} {YEAR_PLT}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f66486",
   "metadata": {},
   "source": [
    "## 3. Data Processing\n",
    "\n",
    "In this section, inconsistencies and missing values are handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a29cf3",
   "metadata": {},
   "source": [
    "### 3.1. Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78b26b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+---------------+------------+-----------+--------+-------+---------------+----------+----+--------------------+-------+--------------+\n",
      "|Driver|DriverNumber|LapTime|LapNumber|Stint|PitOutTime|PitInTime|Sector1Time|Sector2Time|Sector3Time|SpeedI1|SpeedI2|SpeedFL|SpeedST|IsPersonalBest|Compound|TyreLife|FreshTyre|           Team|LapStartTime|TrackStatus|Position|Deleted|FastF1Generated|IsAccurate|Year|           EventName|Session|LapSessionTime|\n",
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+---------------+------------+-----------+--------+-------+---------------+----------+----+--------------------+-------+--------------+\n",
      "|   GAS|          10|   NULL|       45|    3|      NULL|     NULL|       NULL|       NULL|       NULL|   NULL|   NULL|   NULL|   NULL|          NULL|    HARD|      13|     true|     AlphaTauri|    8189.469|        164|    NULL|  false|           true|     false|2022|  Bahrain Grand Prix|      R|      8339.469|\n",
      "|   PER|          11|   NULL|       57|    4|      NULL|     NULL|       NULL|       NULL|       NULL|   NULL|   NULL|   NULL|   NULL|          NULL|    SOFT|      17|    false|Red Bull Racing|    9520.629|         12|    NULL|  false|           true|     false|2022|  Bahrain Grand Prix|      R|      9534.667|\n",
      "|   ALO|          14|   NULL|       36|    2|      NULL|     NULL|       NULL|       NULL|       NULL|   NULL|   NULL|   NULL|   NULL|          NULL|    HARD|      20|     true|         Alpine|    7364.337|        126|    NULL|  false|           true|     false|2022|Saudi Arabian Gra...|      R|      7605.008|\n",
      "|   ALB|          23|   NULL|       48|    2|      NULL|     NULL|       NULL|       NULL|       NULL|   NULL|   NULL|   NULL|   NULL|          NULL|    HARD|      35|     true|       Williams|    8601.107|         12|    NULL|  false|           true|     false|2022|Saudi Arabian Gra...|      R|      8653.292|\n",
      "|   LAT|           6|   NULL|       15|    1|      NULL|     NULL|       NULL|       NULL|       NULL|   NULL|   NULL|   NULL|   NULL|          NULL|  MEDIUM|      15|     true|       Williams|     5132.06|          1|    NULL|  false|           true|     false|2022|Saudi Arabian Gra...|      R|      5223.881|\n",
      "+------+------------+-------+---------+-----+----------+---------+-----------+-----------+-----------+-------+-------+-------+-------+--------------+--------+--------+---------+---------------+------------+-----------+--------+-------+---------------+----------+----+--------------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking laps with no time information\n",
    "lap_data.filter((\n",
    "    col(\"Sector1Time\").isNull() & \n",
    "    col(\"Sector2Time\").isNull() & \n",
    "    col(\"Sector3Time\").isNull()\n",
    ")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55814182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flagging drivers who DNF on the following lap\n",
    "window_spec = Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\").orderBy(\"LapNumber\")\n",
    "\n",
    "lap_data = lap_data.withColumn(\n",
    "    'DNF',\n",
    "    when(\n",
    "        lag(\n",
    "            when(\n",
    "                (col(\"Sector1Time\").isNull() & \n",
    "                 col(\"Sector2Time\").isNull() & \n",
    "                 col(\"Sector3Time\").isNull()),\n",
    "                1\n",
    "            ).otherwise(0),\n",
    "            -1\n",
    "        ).over(window_spec) == 1,\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28fa3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing them rows\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .filter(~(\n",
    "        col(\"Sector1Time\").isNull() & \n",
    "        col(\"Sector2Time\").isNull() & \n",
    "        col(\"Sector3Time\").isNull()\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd9d26",
   "metadata": {},
   "source": [
    "### 3.2. Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deff271",
   "metadata": {},
   "source": [
    "#### 3.2.1. Lap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7e6c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LapTime: 1074\n",
      "PitOutTime: 71889\n",
      "PitInTime: 71860\n",
      "Sector1Time: 1465\n",
      "Sector2Time: 5\n",
      "Sector3Time: 115\n",
      "SpeedI1: 11226\n",
      "SpeedI2: 26\n",
      "SpeedFL: 2629\n",
      "SpeedST: 5998\n"
     ]
    }
   ],
   "source": [
    "# Compute null counts\n",
    "null_counts = lap_data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in lap_data.columns])\n",
    "\n",
    "# Convert to a Row to filter in Python\n",
    "null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# Filter and print only columns with nulls\n",
    "for col_name, count in null_counts_dict.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796c993",
   "metadata": {},
   "source": [
    "**Lap Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "240b8e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "lap_data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0cc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values - compute by subtracting the time at the end and at the start of the lap\n",
    "lap_data = lap_data.withColumn(\"LapTime\", col(\"LapSessionTime\") - col(\"LapStartTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "794ca05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recheck\n",
    "lap_data.filter(col(\"LapTime\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd15e1",
   "metadata": {},
   "source": [
    "**PitOutTime**, **PitInTime** - Does not make sense to fill in these missing values; these features will be used for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a4ae0",
   "metadata": {},
   "source": [
    "**Sector1Time**, **Sector2Time**, **Sector3Time** - These features will be used for feature engineering, no need to fill them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125f791",
   "metadata": {},
   "source": [
    "**SpeedI1**, **SpeedI2**, **SpeedFL**, **SpeedST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4ed130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18057"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "lap_data.filter(\n",
    "    col(\"SpeedI1\").isNull() |\n",
    "    col(\"SpeedI2\").isNull() |\n",
    "    col(\"SpeedFL\").isNull() |\n",
    "    col(\"SpeedST\").isNull()\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "036de2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - speed rolling average\n",
    "driver_lap_window = (\n",
    "    Window\n",
    "    .partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\")\n",
    "    .orderBy(\"LapNumber\")\n",
    "    .rowsBetween(Window.unboundedPreceding, -1)\n",
    ")\n",
    "\n",
    "# List of columns to process\n",
    "speed_cols = [\"SpeedI1\", \"SpeedI2\", \"SpeedFL\", \"SpeedST\"]\n",
    "\n",
    "# Fill missing values\n",
    "for col_name in speed_cols:\n",
    "    cumulative_avg = avg(col(col_name)).over(driver_lap_window)\n",
    "    lap_data = (\n",
    "        lap_data\n",
    "        .withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull(), cumulative_avg).otherwise(col(col_name))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1025ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recheck\n",
    "lap_data.filter(\n",
    "    col(\"SpeedI1\").isNull() |\n",
    "    col(\"SpeedI2\").isNull() |\n",
    "    col(\"SpeedFL\").isNull() |\n",
    "    col(\"SpeedST\").isNull()\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d5cea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - teammate's speed in same lap\n",
    "\n",
    "# Self-join on teammate info\n",
    "teammate_join = lap_data.alias(\"self\").join(\n",
    "    lap_data.alias(\"tm\"),\n",
    "    on=[\n",
    "        col(\"self.Year\") == col(\"tm.Year\"),\n",
    "        col(\"self.EventName\") == col(\"tm.EventName\"),\n",
    "        col(\"self.Session\") == col(\"tm.Session\"),\n",
    "        col(\"self.Team\") == col(\"tm.Team\"),\n",
    "        col(\"self.LapNumber\") == col(\"tm.LapNumber\"),\n",
    "        col(\"self.Driver\") != col(\"tm.Driver\")\n",
    "    ],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Replace missing values from teammate values\n",
    "updated_cols = [\n",
    "    coalesce(col(f\"self.{col_name}\"), col(f\"tm.{col_name}\")).alias(col_name)\n",
    "    if col_name in speed_cols else col(f\"self.{col_name}\")\n",
    "    for col_name in lap_data.columns\n",
    "]\n",
    "\n",
    "lap_data = teammate_join.select(*updated_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "333dae6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recheck\n",
    "lap_data.filter(\n",
    "    col(\"SpeedI1\").isNull() |\n",
    "    col(\"SpeedI2\").isNull() |\n",
    "    col(\"SpeedFL\").isNull() |\n",
    "    col(\"SpeedST\").isNull()\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e87df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values - finish line speed with longest straight speed\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\n",
    "        \"SpeedFL\",\n",
    "        when(col(\"SpeedFL\").isNull(), col(\"SpeedST\")).otherwise(col(\"SpeedFL\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13f842c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recheck\n",
    "lap_data.filter(\n",
    "    col(\"SpeedI1\").isNull() |\n",
    "    col(\"SpeedI2\").isNull() |\n",
    "    col(\"SpeedFL\").isNull() |\n",
    "    col(\"SpeedST\").isNull()\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3e634",
   "metadata": {},
   "source": [
    "#### 3.2.2. Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2a30cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute null counts\n",
    "null_counts = telemetry_data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in telemetry_data.columns])\n",
    "\n",
    "# Convert to a Row to filter in Python\n",
    "null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# Filter and print only columns with nulls\n",
    "for col_name, count in null_counts_dict.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af9c0b",
   "metadata": {},
   "source": [
    "No missing values :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e4f35",
   "metadata": {},
   "source": [
    "#### 3.2.3. Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90cf84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute null counts\n",
    "null_counts = weather_data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in weather_data.columns])\n",
    "\n",
    "# Convert to a Row to filter in Python\n",
    "null_counts_dict = null_counts.first().asDict()\n",
    "\n",
    "# Filter and print only columns with nulls\n",
    "for col_name, count in null_counts_dict.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d64a2",
   "metadata": {},
   "source": [
    "No missing values :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e715afb",
   "metadata": {},
   "source": [
    "## 4. Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de906632",
   "metadata": {},
   "source": [
    "### 4.1. Lap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf73b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define windows\n",
    "start_position_window = Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\")\n",
    "lap_order_window = start_position_window.orderBy(\"LapNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0524bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features\n",
    "lap_data = (\n",
    "    lap_data\n",
    "    .withColumn(\"rolling_avg_laptime\", avg(\"LapTime\").over(lap_order_window.rowsBetween(Window.unboundedPreceding, 0)))\n",
    "    .withColumn(\"pit_in_lap\", when(col(\"PitInTime\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"pit_exit_lap\", when(col(\"PitOutTime\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\n",
    "        \"last_pit_lap\",\n",
    "        coalesce(\n",
    "            max(\"pit_exit_lap\").over(lap_order_window.rowsBetween(Window.unboundedPreceding, 0)),\n",
    "            lit(0)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"laps_since_last_pit\", col(\"LapNumber\") - col(\"last_pit_lap\"))\n",
    "    .withColumn(\n",
    "        \"prev_compound\", \n",
    "        when(\n",
    "            col(\"LapNumber\") == 1, col(\"Compound\")\n",
    "        ).otherwise(\n",
    "            lag(\"Compound\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pit_stop_duration\",\n",
    "        when(\n",
    "            (col(\"PitOutTime\").isNull()) | ((col(\"PitOutTime\").isNotNull()) & (col(\"LapNumber\") == 1)),\n",
    "            lit(0)\n",
    "        ).otherwise(\n",
    "            col(\"PitOutTime\") - lag(\"PitInTime\").over(lap_order_window)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"max_pit_stop_duration\", max(\"pit_stop_duration\").over(lap_order_window))\n",
    "    .withColumn(\"start_position\", first(when(col(\"LapNumber\") == 1, col(\"Position\")), ignorenulls=True).over(start_position_window))\n",
    "    .withColumn(\"position_change_since_race_start\", col(\"start_position\") - col(\"Position\"))\n",
    "    .withColumn(\n",
    "        \"fastest_sector\", when(\n",
    "            (col(\"Sector1Time\") <= col(\"Sector2Time\")) & (col(\"Sector1Time\") <= col(\"Sector3Time\")), 1\n",
    "        ).when(\n",
    "            (col(\"Sector2Time\") <= col(\"Sector1Time\")) & (col(\"Sector2Time\") <= col(\"Sector3Time\")), 2\n",
    "        ).otherwise(3)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07b88680",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_data = lap_data.drop(\n",
    "    \"Sector1Time\", \"Sector2Time\", \"Sector3Time\", \"PitOutTime\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48d5b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------------+---------+-----+---------+-------+-------+-------+-------+--------------+--------+--------+---------+------+------------+-----------+--------+-------+---------------+----------+----+--------------------+-------+--------------+---+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "|Driver|DriverNumber|          LapTime|LapNumber|Stint|PitInTime|SpeedI1|SpeedI2|SpeedFL|SpeedST|IsPersonalBest|Compound|TyreLife|FreshTyre|  Team|LapStartTime|TrackStatus|Position|Deleted|FastF1Generated|IsAccurate|Year|           EventName|Session|LapSessionTime|DNF|rolling_avg_laptime|pit_in_lap|pit_exit_lap|last_pit_lap|laps_since_last_pit|prev_compound|pit_stop_duration|max_pit_stop_duration|start_position|position_change_since_race_start|fastest_sector|\n",
      "+------+------------+-----------------+---------+-----+---------+-------+-------+-------+-------+--------------+--------+--------+---------+------+------------+-----------+--------+-------+---------------+----------+----+--------------------+-------+--------------+---+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "|   ALO|          14|98.42700000000013|        1|    1|     NULL|  284.0|  274.0|  211.0|  277.0|         false|  MEDIUM|       1|     true|Alpine|    3730.161|          1|      10|  false|          false|     false|2022|Abu Dhabi Grand Prix|      R|      3828.588|  0|  98.42700000000013|         0|           0|           0|                  1|       MEDIUM|              0.0|                  0.0|            10|                               0|             3|\n",
      "+------+------------+-----------------+---------+-----+---------+-------+-------+-------+-------+--------------+--------+--------+---------+------+------------+-----------+--------+-------+---------------+----------+----+--------------------+-------+--------------+---+-------------------+----------+------------+------------+-------------------+-------------+-----------------+---------------------+--------------+--------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lap_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd1d0c",
   "metadata": {},
   "source": [
    "### 4.2. Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d332796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window\n",
    "window_spec = Window.partitionBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").orderBy(\"SessionTime\")\n",
    "last_50_window = window_spec.rowsBetween(-49, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0f24cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-lap aggregates\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"avg_speed_last_lap\", avg(\"Speed\").over(window_spec))\n",
    "    .withColumn(\"max_speed_last_lap\", max(\"Speed\").over(window_spec))\n",
    "    .withColumn(\"avg_throttle_last_lap\", avg(\"Throttle\").over(window_spec))\n",
    "    .withColumn(\"avg_brake_last_lap\", avg(\"Brake\").over(window_spec))\n",
    "    .withColumn(\"avg_rpm\", avg(\"RPM\").over(window_spec))\n",
    "    .withColumn(\"gear_change\", when(col(\"nGear\") != lag(\"nGear\").over(window_spec), 1).otherwise(0))\n",
    "    .withColumn(\"gear_change_count\", sum(\"gear_change\").over(window_spec))\n",
    "    .withColumn(\n",
    "        \"DRS_activation_count\",\n",
    "        sum(\n",
    "            when(\n",
    "                (~lag(\"DRS\").over(window_spec).isin(10, 12, 14)) & (col(\"DRS\").isin(10, 12, 14)),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",
    "    )\n",
    "    .withColumn(\"rolling_throttle_mean\", avg(\"Throttle\").over(last_50_window))\n",
    "    .withColumn(\"rolling_brake_intensity\", avg(\"Brake\").over(last_50_window))\n",
    "    .withColumn(\"rolling_gear_change\", when(col(\"nGear\") != lag(\"nGear\").over(window_spec), 1).otherwise(0))\n",
    "    .withColumn(\"rolling_gear_change_rate\", avg(\"rolling_gear_change\").over(last_50_window))\n",
    "    .withColumn(\"rolling_speed_mean\", avg(\"Speed\").over(last_50_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9288123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sector features (define final 5% of distance per lap)\n",
    "max_distance = telemetry_data.groupBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").agg(max(\"Distance\").alias(\"max_dist\"))\n",
    "telemetry_data = telemetry_data.join(max_distance, on=[\"Year\", \"EventName\", \"Driver\", \"LapNumber\"])\n",
    "telemetry_data = telemetry_data.withColumn(\"in_final_sector\", col(\"Distance\") >= col(\"max_dist\") * 0.95)\n",
    "\n",
    "# Define new window\n",
    "final_sector_window = Window.partitionBy(\"Year\", \"EventName\", \"Driver\", \"LapNumber\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "telemetry_data = (\n",
    "    telemetry_data\n",
    "    .withColumn(\"final_sector_avg_speed\", avg(when(col(\"in_final_sector\"), col(\"Speed\"))).over(final_sector_window))\n",
    "    .withColumn(\"final_sector_throttle\", avg(when(col(\"in_final_sector\"), col(\"Throttle\"))).over(final_sector_window))\n",
    "    .withColumn(\"final_sector_brake\", avg(when(col(\"in_final_sector\"), col(\"Brake\"))).over(final_sector_window))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "094b2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telemetry_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d5ac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final per-lap features\n",
    "lap_feature_cols = [\n",
    "    \"EventName\", \"Driver\", \"LapNumber\", \"Year\", \"Session\",\n",
    "    \"avg_speed_last_lap\", \"max_speed_last_lap\",\n",
    "    \"avg_throttle_last_lap\", \"avg_brake_last_lap\",\n",
    "    \"gear_change_count\", \"avg_rpm\", \"DRS_activation_count\",\n",
    "    \"rolling_throttle_mean\", \"rolling_brake_intensity\",\n",
    "    \"rolling_gear_change_rate\", \"rolling_speed_mean\",\n",
    "    \"final_sector_avg_speed\", \"final_sector_throttle\", \n",
    "    \"final_sector_brake\"\n",
    "]\n",
    "\n",
    "# For all columns, take the FIRST value per (Driver, LapNumber)\n",
    "# Because window functions already populated each row with the same value within each lap\n",
    "aggregated_laps = (\n",
    "    telemetry_data\n",
    "    .select(*lap_feature_cols)\n",
    "    .groupBy(\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\")\n",
    "    .agg(*[\n",
    "        first(col_name).alias(col_name) \n",
    "        if col_name != \"DRS_activation_count\" \n",
    "        else last(col_name).alias(col_name) \n",
    "        for col_name in lap_feature_cols \n",
    "        if col_name not in (\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbd107",
   "metadata": {},
   "source": [
    "### 4.3. Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08776414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join weather and lap data on session keys\n",
    "joined = weather_data.alias(\"w\").join(\n",
    "    lap_data.select(\n",
    "        \"LapNumber\", \"LapStartTime\", \"LapSessionTime\", \"Year\", \"EventName\", \"Session\"\n",
    "    ).alias(\"l\"),\n",
    "    on=[\n",
    "        col(\"w.Year\") == col(\"l.Year\"),\n",
    "        col(\"w.EventName\") == col(\"l.EventName\"),\n",
    "        col(\"w.Session\") == col(\"l.Session\")\n",
    "    ],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Filter to keep only rows where weather Time is inside lap range\n",
    "filtered = joined.filter(\n",
    "    (col(\"w.Time\") >= col(\"l.LapStartTime\")) &\n",
    "    (col(\"w.Time\") <= col(\"l.LapSessionTime\"))\n",
    ")\n",
    "\n",
    "# Step 3: Select columns and reattach to full weather_data\n",
    "weather_data = filtered.select(\n",
    "    col(\"w.*\"), \n",
    "    col(\"l.LapNumber\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f79390e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window\n",
    "# window_spec = Window.partitionBy(\"Year\", \"EventName\", \"LapNumber\").orderBy(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93acc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-lap aggregates\n",
    "weather_data = (\n",
    "    weather_data\n",
    "    .groupBy(\"Year\", \"EventName\", \"Session\", \"LapNumber\")\n",
    "    .agg(\n",
    "        avg(\"AirTemp\").alias(\"avg_air_temp\"),\n",
    "        avg(\"Humidity\").alias(\"avg_humidity\"),\n",
    "        avg(\"Pressure\").alias(\"avg_pressure\"),\n",
    "        max(\"Rainfall\").alias(\"max_rainfall\"),\n",
    "        avg(\"TrackTemp\").alias(\"avg_track_temp\"),\n",
    "        avg(\"WindDirection\").alias(\"avg_wind_direction\"),\n",
    "        avg(\"WindSpeed\").alias(\"avg_wind_speed\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c6a39",
   "metadata": {},
   "source": [
    "### Joining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4999694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join lap_data and telemetry_data\n",
    "data = (\n",
    "    lap_data.alias('lap')\n",
    "    .join(\n",
    "        aggregated_laps.alias('telemetry'),\n",
    "        on=[\"Year\", \"EventName\", \"Session\", \"Driver\", \"LapNumber\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        weather_data.alias('weather'),  # <- use the aggregated weather data\n",
    "        on=[\"Year\", \"EventName\", \"Session\", \"LapNumber\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec477f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable\n",
    "data = (\n",
    "    data\n",
    "    .withColumn(\n",
    "        \"WillPitNextLap\", when(\n",
    "            lead(\"PitInTime\", 1).over(Window.partitionBy(\"Year\", \"EventName\", \"Session\", \"Driver\").orderBy(\"LapNumber\")).isNotNull(), 1\n",
    "        )\n",
    "    .otherwise(0)\n",
    "    .cast(IntegerType())\n",
    "    )\n",
    ")\n",
    "\n",
    "data = data.drop(\"PitInTime\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65edc10d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9271ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef11b74",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ccad9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'EventName',\n",
       " 'Session',\n",
       " 'LapNumber',\n",
       " 'Driver',\n",
       " 'DriverNumber',\n",
       " 'LapTime',\n",
       " 'Stint',\n",
       " 'SpeedI1',\n",
       " 'SpeedI2',\n",
       " 'SpeedFL',\n",
       " 'SpeedST',\n",
       " 'IsPersonalBest',\n",
       " 'Compound',\n",
       " 'TyreLife',\n",
       " 'FreshTyre',\n",
       " 'Team',\n",
       " 'LapStartTime',\n",
       " 'TrackStatus',\n",
       " 'Position',\n",
       " 'Deleted',\n",
       " 'FastF1Generated',\n",
       " 'IsAccurate',\n",
       " 'LapSessionTime',\n",
       " 'DNF',\n",
       " 'rolling_avg_laptime',\n",
       " 'pit_in_lap',\n",
       " 'pit_exit_lap',\n",
       " 'last_pit_lap',\n",
       " 'laps_since_last_pit',\n",
       " 'prev_compound',\n",
       " 'pit_stop_duration',\n",
       " 'max_pit_stop_duration',\n",
       " 'start_position',\n",
       " 'position_change_since_race_start',\n",
       " 'fastest_sector',\n",
       " 'avg_speed_last_lap',\n",
       " 'max_speed_last_lap',\n",
       " 'avg_throttle_last_lap',\n",
       " 'avg_brake_last_lap',\n",
       " 'gear_change_count',\n",
       " 'avg_rpm',\n",
       " 'DRS_activation_count',\n",
       " 'rolling_throttle_mean',\n",
       " 'rolling_brake_intensity',\n",
       " 'rolling_gear_change_rate',\n",
       " 'rolling_speed_mean',\n",
       " 'final_sector_avg_speed',\n",
       " 'final_sector_throttle',\n",
       " 'final_sector_brake',\n",
       " 'avg_air_temp',\n",
       " 'avg_humidity',\n",
       " 'avg_pressure',\n",
       " 'max_rainfall',\n",
       " 'avg_track_temp',\n",
       " 'avg_wind_direction',\n",
       " 'avg_wind_speed',\n",
       " 'WillPitNextLap']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "585f3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Session\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d83db5f9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Define window spec to partition by Year, EventName, Driver and order by LapNumber\n",
    "window_spec = Window.partitionBy(\"Year\", \"EventName\", \"Driver\").orderBy(\"LapNumber\")\n",
    "\n",
    "# Check if the driver will pit in any of the next 3 laps\n",
    "data_with_shifted = data.withColumn(\"WillPitIn3Laps_shifted_1\", F.lead(\"WillPitNextLap\", 1).over(window_spec)) \\\n",
    "                    .withColumn(\"WillPitIn3Laps_shifted_2\", F.lead(\"WillPitNextLap\", 2).over(window_spec)) \\\n",
    "                    .withColumn(\"WillPitIn3Laps_shifted_3\", F.lead(\"WillPitNextLap\", 3).over(window_spec))\n",
    "\n",
    "# Update WillPitNextLap based on the next 3 laps' values\n",
    "data_updated = data_with_shifted.withColumn(\n",
    "    \"WillPitNextLap\", \n",
    "    F.when(\n",
    "        (F.col(\"WillPitIn3Laps_shifted_1\") == True) |\n",
    "        (F.col(\"WillPitIn3Laps_shifted_2\") == True) |\n",
    "        (F.col(\"WillPitIn3Laps_shifted_3\") == True), \n",
    "        1\n",
    "    ).otherwise(F.col(\"WillPitNextLap\"))\n",
    ")\n",
    "\n",
    "# Drop the temporary shifted columns\n",
    "data = data_updated.drop(\"WillPitIn3Laps_shifted_1\", \"WillPitIn3Laps_shifted_2\", \"WillPitIn3Laps_shifted_3\")\n",
    "\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6b01c67",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Train: all 2023 races except the last 4\n",
    "train_data = data.filter(\n",
    "    ~((col(\"EventName\").isin(\"Abu Dhabi Grand Prix\", \n",
    "                             \"Las Vegas Grand Prix\", \n",
    "                             \"Mexico City Grand Prix\", \n",
    "                             \"SÃ£o Paulo Grand Prix\",\n",
    "                             \"United States Grand Prix\",\n",
    "                             \"Qatar Grand Prix\")) & \n",
    "      (col(\"Year\") == 2024))\n",
    ")\n",
    "\n",
    "# Validation: 3 races before final\n",
    "val_data = data.filter(\n",
    "    ((col(\"EventName\").isin(\"United States Grand Prix\", \n",
    "                            \"Mexico City Grand Prix\", \n",
    "                            \"SÃ£o Paulo Grand Prix\")) & \n",
    "     (col(\"Year\") == 2024))\n",
    ")\n",
    "\n",
    "# Test: final race\n",
    "test_data = data.filter(\n",
    "    ((col(\"EventName\").isin(\"Las Vegas Grand Prix\", \n",
    "                            \"Qatar Grand Prix\", \n",
    "                            \"Abu Dhabi Grand Prix\")) & \n",
    "     (col(\"Year\") == 2024))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a89c620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'EventName',\n",
       " 'LapNumber',\n",
       " 'Driver',\n",
       " 'DriverNumber',\n",
       " 'LapTime',\n",
       " 'Stint',\n",
       " 'SpeedI1',\n",
       " 'SpeedI2',\n",
       " 'SpeedFL',\n",
       " 'SpeedST',\n",
       " 'IsPersonalBest',\n",
       " 'Compound',\n",
       " 'TyreLife',\n",
       " 'FreshTyre',\n",
       " 'Team',\n",
       " 'LapStartTime',\n",
       " 'TrackStatus',\n",
       " 'Position',\n",
       " 'Deleted',\n",
       " 'FastF1Generated',\n",
       " 'IsAccurate',\n",
       " 'LapSessionTime',\n",
       " 'DNF',\n",
       " 'rolling_avg_laptime',\n",
       " 'pit_in_lap',\n",
       " 'pit_exit_lap',\n",
       " 'last_pit_lap',\n",
       " 'laps_since_last_pit',\n",
       " 'prev_compound',\n",
       " 'pit_stop_duration',\n",
       " 'max_pit_stop_duration',\n",
       " 'start_position',\n",
       " 'position_change_since_race_start',\n",
       " 'fastest_sector',\n",
       " 'avg_speed_last_lap',\n",
       " 'max_speed_last_lap',\n",
       " 'avg_throttle_last_lap',\n",
       " 'avg_brake_last_lap',\n",
       " 'gear_change_count',\n",
       " 'avg_rpm',\n",
       " 'DRS_activation_count',\n",
       " 'rolling_throttle_mean',\n",
       " 'rolling_brake_intensity',\n",
       " 'rolling_gear_change_rate',\n",
       " 'rolling_speed_mean',\n",
       " 'final_sector_avg_speed',\n",
       " 'final_sector_throttle',\n",
       " 'final_sector_brake',\n",
       " 'avg_air_temp',\n",
       " 'avg_humidity',\n",
       " 'avg_pressure',\n",
       " 'max_rainfall',\n",
       " 'avg_track_temp',\n",
       " 'avg_wind_direction',\n",
       " 'avg_wind_speed',\n",
       " 'WillPitNextLap']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2808f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Driver|\n",
      "+------+\n",
      "|   OCO|\n",
      "|   BOT|\n",
      "|   HAM|\n",
      "|   MSC|\n",
      "|   VER|\n",
      "|   ZHO|\n",
      "|   MAG|\n",
      "|   SAR|\n",
      "|   NOR|\n",
      "|   TSU|\n",
      "|   HUL|\n",
      "|   ALB|\n",
      "|   PER|\n",
      "|   STR|\n",
      "|   LAW|\n",
      "|   GAS|\n",
      "|   LEC|\n",
      "|   DEV|\n",
      "|   RUS|\n",
      "|   COL|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Driver\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd45da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98830ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: all 2023 races except the last 4\n",
    "# Test: final race\n",
    "train_data = data.filter(\n",
    "    ((col(\"EventName\").isin(\"Abu Dhabi Grand Prix\", )) & \n",
    "     (col(\"Year\") == 2022))\n",
    ")\n",
    "\n",
    "# Validation: 3 races before final\n",
    "val_data = data.filter(\n",
    "    ((col(\"EventName\").isin(\"Abu Dhabi Grand Prix\",)) & \n",
    "     (col(\"Year\") == 2023))\n",
    ")\n",
    "\n",
    "# Test: final race\n",
    "test_data = data.filter(\n",
    "    ((col(\"EventName\").isin(\"Abu Dhabi Grand Prix\")) & \n",
    "     (col(\"Year\") == 2024))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cf7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Team: 10\n",
      "Unique values in Compound: 3\n",
      "Unique values in Driver: 19\n",
      "Unique values in EventName: 1\n",
      "Unique values in prev_compound: 3\n",
      "Training on laps â‰¤ 5, predicting lap 6\n",
      "Lap 6 AUPRC: 0.0000\n",
      "Training on laps â‰¤ 6, predicting lap 7\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Filter for a single race\n",
    "train_data = data.filter(\n",
    "    (col(\"EventName\") == \"Abu Dhabi Grand Prix\") & \n",
    "    (col(\"Year\") == 2024)\n",
    ")\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "categorical_cols = [\"Team\", \"Compound\", \"Driver\", \"EventName\", \"prev_compound\"]\n",
    "for cat_col in categorical_cols:\n",
    "    unique_count = train_data.select(cat_col).distinct().count()\n",
    "    print(f\"Unique values in {cat_col}: {unique_count}\")\n",
    "\n",
    "# Define preprocessing\n",
    "indexers_and_encoders = [\n",
    "    StringIndexer(inputCol=\"Team\", outputCol=\"TeamIndex\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Compound\", outputCol=\"CompoundIndex\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Driver\", outputCol=\"DriverIndex\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"EventName\", outputCol=\"EventNameIndex\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"prev_compound\", outputCol=\"prev_compound_index\", handleInvalid=\"keep\"),\n",
    "    OneHotEncoder(inputCol=\"CompoundIndex\", outputCol=\"CompoundIndex_ohe\"),\n",
    "    OneHotEncoder(inputCol=\"prev_compound_index\", outputCol=\"prev_compound_ohe\"),\n",
    "    OneHotEncoder(inputCol=\"DriverIndex\", outputCol=\"DriverIndex_ohe\")\n",
    "]\n",
    "\n",
    "def expand_one_hot_vectors(df, ohe_columns):\n",
    "    for vec_col in ohe_columns:\n",
    "        df = df.withColumn(f\"{vec_col}_array\", vector_to_array(col(vec_col)))\n",
    "        size = len(df.select(vec_col).first()[0])\n",
    "        for i in range(size):\n",
    "            df = df.withColumn(f\"{vec_col}_{i}\", col(f\"{vec_col}_array\")[i])\n",
    "    return df.drop(*[f\"{col}_array\" for col in ohe_columns])\n",
    "\n",
    "# Store AUPRC scores\n",
    "auprc_by_lap = {}\n",
    "\n",
    "# Get max lap number\n",
    "max_lap = train_data.agg({\"LapNumber\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Start from lap 5\n",
    "for lap in range(5, max_lap):\n",
    "    print(f\"Training on laps â‰¤ {lap}, predicting lap {lap + 1}\")\n",
    "\n",
    "    train_subset = train_data.filter(col(\"LapNumber\") <= lap)\n",
    "    test_subset = train_data.filter(col(\"LapNumber\") == lap + 1)\n",
    "\n",
    "    if test_subset.count() == 0:\n",
    "        print(f\"No data for lap {lap+1}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Fit and transform pipeline\n",
    "    pipeline = Pipeline(stages=indexers_and_encoders)\n",
    "    model = pipeline.fit(train_subset)\n",
    "\n",
    "    train_transformed = model.transform(train_subset)\n",
    "    test_transformed = model.transform(test_subset)\n",
    "\n",
    "    # Expand OHE\n",
    "    ohe_columns = [\"CompoundIndex_ohe\", \"prev_compound_ohe\", \"DriverIndex_ohe\"]\n",
    "    train_transformed = expand_one_hot_vectors(train_transformed, ohe_columns)\n",
    "    test_transformed = expand_one_hot_vectors(test_transformed, ohe_columns)\n",
    "\n",
    "    # Assemble features\n",
    "    feature_cols = [c for c in train_transformed.columns if c.startswith(\"CompoundIndex_ohe_\") or \n",
    "                    c.startswith(\"prev_compound_ohe_\") or \n",
    "                    c.startswith(\"DriverIndex_ohe_\") or \n",
    "                    c in [\"TeamIndex\"]]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_assembled = assembler.transform(train_transformed)\n",
    "    test_assembled = assembler.transform(test_transformed)\n",
    "\n",
    "    # Train classifier\n",
    "    rf = RandomForestClassifier(labelCol=\"WillPitNextLap\", featuresCol=\"features\", maxBins=64)\n",
    "    rf_model = rf.fit(train_assembled)\n",
    "\n",
    "    # Predict\n",
    "    predictions = rf_model.transform(test_assembled)\n",
    "\n",
    "    # Evaluate AUPRC\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"WillPitNextLap\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "    auprc = evaluator.evaluate(predictions)\n",
    "    auprc_by_lap[lap + 1] = auprc\n",
    "\n",
    "    print(f\"Lap {lap + 1} AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4348b34b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import optuna\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col\n",
    "import uuid\n",
    "import numpy as np\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def expand_one_hot_vectors(df, ohe_columns):\n",
    "    \"\"\"Expand one-hot encoded vector columns into individual binary columns.\"\"\"\n",
    "    for vec_col in ohe_columns:\n",
    "        # Convert vector to array\n",
    "        df = df.withColumn(f\"{vec_col}_array\", vector_to_array(col(vec_col)))\n",
    "        \n",
    "        # Get size of the vector (number of categories)\n",
    "        size = len(df.select(vec_col).first()[0])\n",
    "        \n",
    "        # Create individual columns for each category\n",
    "        for i in range(size):\n",
    "            df = df.withColumn(f\"{vec_col}_{i}\", col(f\"{vec_col}_array\")[i])\n",
    "    \n",
    "    return df.drop(*[f\"{col}_array\" for col in ohe_columns])\n",
    "\n",
    "def train_model(target, train_data, val_data, test_data, model_type, optimize, num_features=20, n_trials=10, random_state=None):\n",
    "    \"\"\"\n",
    "    Train a model with optional Optuna hyperparameter optimization and L1-based feature selection for LR/MLP.\n",
    "    \n",
    "    Args:\n",
    "        target (str): Target column name.\n",
    "        train_data: PySpark DataFrame for training.\n",
    "        val_data: PySpark DataFrame for validation during optimization.\n",
    "        test_data: PySpark DataFrame for final evaluation.\n",
    "        model_type (str): Model type ('gbt', 'rf', 'lr', 'mlp').\n",
    "        optimize (bool): If True, optimize hyperparameters with Optuna; if False, use defaults.\n",
    "        num_features (int): Number of features to select for LR/MLP using L1 regularization.\n",
    "        n_trials (int): Number of Optuna trials if optimize=True.\n",
    "        random_state (int or None): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained model, areaUnderPR score on test data)\n",
    "    \"\"\"\n",
    "    # Validate model_type\n",
    "    valid_models = ['gbt', 'rf', 'lr', 'mlp']\n",
    "    if model_type not in valid_models:\n",
    "        raise ValueError(f\"model_type must be one of {valid_models}\")\n",
    "\n",
    "    # Define indexers and encoders for the relevant columns\n",
    "    indexers_and_encoders = [\n",
    "        # Indexers\n",
    "        StringIndexer(inputCol=\"Team\", outputCol=\"TeamIndex\", handleInvalid=\"keep\"),\n",
    "        StringIndexer(inputCol=\"Compound\", outputCol=\"CompoundIndex\", handleInvalid=\"keep\"),\n",
    "        StringIndexer(inputCol=\"Driver\", outputCol=\"DriverIndex\", handleInvalid=\"keep\"),\n",
    "        StringIndexer(inputCol=\"EventName\", outputCol=\"EventNameIndex\", handleInvalid=\"keep\"),\n",
    "        StringIndexer(inputCol=\"prev_compound\", outputCol=\"prev_compound_index\", handleInvalid=\"keep\"),\n",
    "        # OneHot Encoders\n",
    "        OneHotEncoder(inputCol=\"CompoundIndex\", outputCol=\"CompoundIndex_ohe\"),\n",
    "        OneHotEncoder(inputCol=\"prev_compound_index\", outputCol=\"prev_compound_ohe\")\n",
    "    ]\n",
    "\n",
    "    # Calculate class weights\n",
    "    label_counts = train_data.groupBy(target).count()\n",
    "    total_count = label_counts.agg({\"count\": \"sum\"}).collect()[0][0]  \n",
    "    label_dist = {row[target]: row['count'] for row in label_counts.collect()}\n",
    "    num_classes = len(label_dist)\n",
    "    if not label_dist:\n",
    "        raise ValueError(\"No labels found in the target column\")\n",
    "    class_weights = {label: total_count / (num_classes * count) for label, count in label_dist.items()}\n",
    "    print(\"Label distribution:\", label_dist)  \n",
    "\n",
    "    # Add weightCol\n",
    "    def add_weights(df):\n",
    "        expr = None\n",
    "        for label, weight in class_weights.items():\n",
    "            condition = when(col(target) == float(label), float(weight))\n",
    "            expr = condition if expr is None else expr.when(col(target) == float(label), float(weight))\n",
    "        expr = expr.otherwise(1.0)\n",
    "        return df.withColumn(\"weightCol\", expr)\n",
    "    \n",
    "    train_data = add_weights(train_data) \n",
    "    val_data = val_data.withColumn(\"weightCol\", lit(1.0)) \n",
    "    test_data = test_data.withColumn(\"weightCol\", lit(1.0)) \n",
    "    \n",
    "\n",
    "    # Cache data\n",
    "    train_data.cache()\n",
    "    val_data.cache()\n",
    "    test_data.cache()\n",
    "\n",
    "    # Apply indexers_and_encoders to all datasets (fit on train_data only)\n",
    "    indexer_pipeline = Pipeline(stages=indexers_and_encoders)\n",
    "    indexer_model = indexer_pipeline.fit(train_data)\n",
    "    train_data = indexer_model.transform(train_data)\n",
    "    val_data = indexer_model.transform(val_data)\n",
    "    test_data = indexer_model.transform(test_data)\n",
    "\n",
    "    # Get all columns for feature selection (exclude target, raw categorical columns)\n",
    "    categorical_cols = [\"Team\", \"Compound\", \"Driver\", \"EventName\", \"prev_compound\", \"CompoundIndex\", \"prev_compound_index\"]\n",
    "    feature_cols = [col for col in train_data.columns if col != target and col not in categorical_cols]\n",
    "\n",
    "    # Perform L1-based feature selection for lr and mlp\n",
    "    selected_feature_names = feature_cols  # Default: use all features\n",
    "    if model_type in ['lr', 'mlp']:\n",
    "        # First expand one-hot encoded vectors\n",
    "        ohe_columns = [\"CompoundIndex_ohe\", \"prev_compound_ohe\"]  # Add any other OHE columns\n",
    "        train_expanded = expand_one_hot_vectors(train_data, ohe_columns)\n",
    "        val_expanded = expand_one_hot_vectors(val_data, ohe_columns)\n",
    "        test_expanded = expand_one_hot_vectors(test_data, ohe_columns)\n",
    "        \n",
    "        # Get all feature columns (now including expanded binary columns)\n",
    "        feature_cols = [col for col in train_expanded.columns \n",
    "                       if col != target \n",
    "                       and col not in categorical_cols \n",
    "                       and not col.endswith(\"_array\")]\n",
    "        \n",
    "        # Create temporary assembler for all features\n",
    "        temp_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"temp_l1_features\")\n",
    "        temp_scaler = StandardScaler(inputCol=\"temp_l1_features\", outputCol=\"temp_scaled_features\", withMean=True, withStd=True)\n",
    "        \n",
    "        # Fit an L1-regularized LogisticRegression\n",
    "        l1_lr = LogisticRegression(\n",
    "            labelCol=target,\n",
    "            featuresCol=\"temp_scaled_features\",\n",
    "            regParam=0.1,\n",
    "            elasticNetParam=0.5,\n",
    "            maxIter=100,\n",
    "            seed=random_state\n",
    "        )\n",
    "        l1_pipeline = Pipeline(stages=[temp_assembler, temp_scaler, l1_lr])\n",
    "        l1_model = l1_pipeline.fit(train_expanded)\n",
    "        \n",
    "        # Extract coefficients\n",
    "        coefficients = np.abs(l1_model.stages[-1].coefficients.toArray())\n",
    "        \n",
    "        # Safely select top features\n",
    "        non_zero_indices = np.where(coefficients > 0)[0]\n",
    "        if len(non_zero_indices) > 0:\n",
    "            top_n = min(num_features, len(non_zero_indices))\n",
    "            top_indices = np.argsort(coefficients[non_zero_indices])[::-1][:top_n]\n",
    "            selected_feature_names = [feature_cols[non_zero_indices[i]] for i in top_indices]\n",
    "        else:\n",
    "            selected_feature_names = feature_cols[:num_features]\n",
    "            \n",
    "        print(f\"Selected {len(selected_feature_names)} features for {model_type}: {selected_feature_names}\")\n",
    "        \n",
    "        # Clean up temporary columns after feature selection\n",
    "        train_data = train_expanded.drop(\"temp_l1_features\", \"temp_scaled_features\")\n",
    "        val_data = val_expanded.drop(\"temp_l1_features\", \"temp_scaled_features\")\n",
    "        test_data = test_expanded.drop(\"temp_l1_features\", \"temp_scaled_features\")\n",
    "\n",
    "    # Create assembler with selected features\n",
    "    assembler = VectorAssembler(inputCols=selected_feature_names, outputCol=\"features\")\n",
    "    feature_pipeline = Pipeline(stages=[assembler])\n",
    "\n",
    "    # Update datasets with selected features\n",
    "    feature_model = feature_pipeline.fit(train_data)\n",
    "    train_data = feature_model.transform(train_data)\n",
    "    val_data = feature_model.transform(val_data)\n",
    "    test_data = feature_model.transform(test_data)\n",
    "\n",
    "    def get_classifier(trial=None):\n",
    "        \"\"\"Define classifier based on model_type and optional trial parameters.\"\"\"\n",
    "        if model_type == 'rf':\n",
    "            if optimize and trial:\n",
    "                num_trees = trial.suggest_int(\"numTrees\", 10, 100)\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 5, 30)\n",
    "                min_instances_per_node = trial.suggest_int(\"minInstancesPerNode\", 1, 10)\n",
    "                subsampling_rate = trial.suggest_float(\"subsamplingRate\", 0.5, 1.0)\n",
    "                return RandomForestClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"features\",\n",
    "                    weightCol=\"weightCol\",\n",
    "                    numTrees=num_trees,\n",
    "                    maxDepth=max_depth,\n",
    "                    minInstancesPerNode=min_instances_per_node,\n",
    "                    subsamplingRate=subsampling_rate,\n",
    "                    seed=random_state\n",
    "                )\n",
    "            return RandomForestClassifier(labelCol=target, featuresCol=\"features\", weightCol=\"weightCol\", seed=random_state)\n",
    "        \n",
    "        elif model_type == 'gbt':\n",
    "            if optimize and trial:\n",
    "                max_depth = trial.suggest_int(\"maxDepth\", 3, 10)\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 10, 100)\n",
    "                step_size = trial.suggest_float(\"stepSize\", 0.01, 0.3)\n",
    "                subsampling_rate = trial.suggest_float(\"subsamplingRate\", 0.5, 1.0)\n",
    "                return GBTClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"features\",\n",
    "                    maxDepth=max_depth,\n",
    "                    maxIter=max_iter,\n",
    "                    stepSize=step_size,\n",
    "                    subsamplingRate=subsampling_rate,\n",
    "                    seed=random_state\n",
    "                )\n",
    "            return GBTClassifier(labelCol=target, featuresCol=\"features\",seed=random_state)\n",
    "        \n",
    "        elif model_type == 'lr':\n",
    "            if optimize and trial:\n",
    "                reg_param = trial.suggest_float(\"regParam\", 1e-5, 0.5, log=True)\n",
    "                elastic_net = trial.suggest_float(\"elasticNetParam\", 0.0, 1.0)\n",
    "                tol = trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 300)\n",
    "                fit_intercept = trial.suggest_categorical(\"fitIntercept\", [True, False])\n",
    "                return LogisticRegression(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    weightCol=\"weightCol\",\n",
    "                    regParam=reg_param,\n",
    "                    elasticNetParam=elastic_net,\n",
    "                    tol=tol,\n",
    "                    maxIter=max_iter,\n",
    "                    fitIntercept=fit_intercept,\n",
    "                    seed=random_state\n",
    "                )\n",
    "            return LogisticRegression(labelCol=target, featuresCol=\"scaled_features\", weightCol=\"weightCol\",seed=random_state)\n",
    "        \n",
    "        elif model_type == 'mlp':\n",
    "            layers = [train_data.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"], 64, 32, 2]\n",
    "            if optimize and trial:\n",
    "                max_iter = trial.suggest_int(\"maxIter\", 50, 200)\n",
    "                block_size = trial.suggest_int(\"blockSize\", 32, 128)\n",
    "                step_size = trial.suggest_float(\"stepSize\", 0.001, 0.1, log=True)\n",
    "                return MultilayerPerceptronClassifier(\n",
    "                    labelCol=target,\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    layers=layers,\n",
    "                    maxIter=max_iter,\n",
    "                    blockSize=block_size,\n",
    "                    stepSize=step_size,\n",
    "                    seed=random_state\n",
    "                )\n",
    "            return MultilayerPerceptronClassifier(labelCol=target, featuresCol=\"scaled_features\", layers=layers, seed=random_state\n",
    "            )\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Objective function for Optuna\"\"\"\n",
    "        classifier = get_classifier(trial)\n",
    "        stages = []\n",
    "        \n",
    "        # Add feature assembler (only if features column doesn't exist)\n",
    "        if \"features\" not in train_data.columns:\n",
    "            stages.append(assembler)\n",
    "\n",
    "        # Add scaler for lr/mlp\n",
    "        if model_type in ['lr', 'mlp']:\n",
    "            stages.append(StandardScaler(\n",
    "                inputCol=\"features\",\n",
    "                outputCol=\"scaled_features\",\n",
    "                withMean=True,\n",
    "                withStd=True\n",
    "            ))\n",
    "        stages.append(classifier)\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        try:\n",
    "            model = pipeline.fit(train_data)\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "            auc = evaluator.evaluate(model.transform(val_data))\n",
    "            return auc\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    # Train model\n",
    "    if optimize:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"{model_type}_optimization_{uuid.uuid4()}\"\n",
    "        )\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
    "        print(f\"Best areaUnderPR: {study.best_value}\")\n",
    "        classifier = get_classifier(study.best_trial)\n",
    "    else:\n",
    "        classifier = get_classifier()\n",
    "\n",
    "    # Build pipeline\n",
    "    stages = [assembler]\n",
    "    if model_type in ['lr', 'mlp']:\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        stages.append(scaler)\n",
    "    stages.append(classifier)\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    for name, df in [('train', train_data), ('val', val_data), ('test', test_data)]:\n",
    "        for col_name in ['features', 'scaled_features']:\n",
    "            if col_name in df.columns:\n",
    "                df = df.drop(col_name)\n",
    "        if name == 'train':\n",
    "            train_data = df\n",
    "        elif name == 'val':\n",
    "            val_data = df\n",
    "        else:\n",
    "            test_data = df\n",
    "    \n",
    "    # Fit and evaluate on test data\n",
    "\n",
    "    # Combine train and validation data\n",
    "    final_train_data = train_data.unionByName(val_data)\n",
    "\n",
    "    # Fit model on combined training data\n",
    "    model = pipeline.fit(train_data)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\")\n",
    "    areaUnderPR_test = evaluator.evaluate(model.transform(test_data))\n",
    "\n",
    "    train_data = train_data.withColumn(\"weightCol\", F.lit(1))\n",
    "    areaUnderPR_train = evaluator.evaluate(model.transform(train_data))\n",
    "    \n",
    "    # Unpersist data\n",
    "    train_data.unpersist()\n",
    "    val_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "    \n",
    "    return model, areaUnderPR_train, areaUnderPR_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00305572",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "model, areaUnderPR_train, areaUnderPR_test = train_model(\n",
    "    target=\"WillPitNextLap\",\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    model_type=\"rf\",\n",
    "    optimize=False,\n",
    "    num_features=20,\n",
    "    n_trials=3,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Model areaUnderPR on train data: {areaUnderPR_train}\")\n",
    "print(f\"Model areaUnderPR on test data: {areaUnderPR_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
